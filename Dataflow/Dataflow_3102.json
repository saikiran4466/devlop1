{"dataflow":{"dfKey":"2c62844b-0e58-4f0d-8b12-4e6d039e044b","name":"Dataflow_3102","tags":null,"description":null,"definition":"{\"name\":\"Dataflow_3102\",\"version\":0,\"livyServerId\":0,\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[\"GenerateNumber\"],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"partitionOptions\":{\"partitionType\":\"\",\"name\":\"\"},\"easyQueryDefJson\":{\"sqlQuery\":\"\"},\"componentId\":1,\"componentName\":\"SQL 1\",\"tableName\":\"SQL_1\",\"category\":\"Processor\",\"componentType\":\"SQL\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"dbTable\":\"\"},\"className\":\"com.datagaps.dataflow.models.SQLComponent\",\"isCheckpointEnabled\":\"N\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"import pandas as pd\\r\\ndef python_function(arg):\\r\\n    python_df\\u003dspark.sql(\\\"select * from jdbc_3\\\")\\r\\n    python_df.createOrReplaceTempView(\\\"python_df_name\\\")\\r\\n    python_df.show()\\r\\n    return python_df\\r\\n    \\r\\n    sum_result \\u003d python_function(\\\"textt\\\")\\r\\n    print(sum_result.show())\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":2,\"componentName\":\"Code 2\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"partitionOptions\":{\"partitionType\":\"\",\"name\":\"\"},\"enableTrim\":\"N\",\"easyQueryDefJson\":{\"sqlQuery\":\"\"},\"componentId\":3,\"componentName\":\"JDBC 3\",\"tableName\":\"JDBC_3\",\"category\":\"Source\",\"componentType\":\"JDBC\",\"rank\":0,\"dataSourceName\":\"Oracle_102_rm_test\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"dbTable\":\"select COUNTRY,C_DATE,C_TIMESTAMP,DEATHS,CONFIRMED_CASES from SH.DA_COVID1COPY1\"},\"className\":\"com.datagaps.dataflow.models.JDBCComponent\",\"isCheckpointEnabled\":\"N\",\"dataSourceLogicalName\":\"\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"partitionOptions\":{\"partitionType\":\"\",\"name\":\"\"},\"easyQueryDefJson\":{\"sqlQuery\":\"\"},\"componentId\":4,\"componentName\":\"SQL 4\",\"tableName\":\"SQL_4\",\"category\":\"Processor\",\"componentType\":\"SQL\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"dbTable\":\"select * from jdbc_3\"},\"className\":\"com.datagaps.dataflow.models.SQLComponent\",\"isCheckpointEnabled\":\"N\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"import org.apache.spark.sql.functions.udf\\r\\nimport org.apache.spark.sql.functions.col\\r\\nimport org.apache.spark.sql.{Row, SparkSession}\\r\\n\\r\\n  import spark.implicits._\\r\\n  val columns \\u003d Seq(\\\"Seqno\\\",\\\"Quote\\\")\\r\\n  val data \\u003d Seq((\\\"1\\\", \\\"Be the change that you wish to see in the world\\\"),\\r\\n    (\\\"2\\\", \\\"Everyone thinks of changing the world, but no one thinks of changing himself.\\\"),\\r\\n    (\\\"3\\\", \\\"The purpose of our lives is to be happy.\\\")\\r\\n\\r\\n  )\\r\\n  val df \\u003d data.toDF(columns:_*)\\r\\n  df.show(false)\\r\\n\\r\\n  val convertCase \\u003d  (str:String) \\u003d\\u003e {\\r\\n    val arr \\u003d str.split(\\\" \\\")\\r\\n    arr.map(f\\u003d\\u003e  f.substring(0,1).toUpperCase + f.substring(1,f.length)).mkString(\\\" \\\")\\r\\n  }\\r\\n\\r\\n  //Using with DataFrame\\r\\n  val convertUDF \\u003d udf(convertCase)\\r\\n  df.select(col(\\\"Seqno\\\"),\\r\\n    convertUDF(col(\\\"Quote\\\")).as(\\\"Quote\\\") ).show(false)\\r\\n\\r\\n  // Using it on SQL\\r\\n  spark.udf.register(\\\"convertUDF\\\", convertCase)\\r\\n  df.createOrReplaceTempView(\\\"QUOTE_TABLE\\\")\\r\\n  spark.sql(\\\"select Seqno, convertUDF(Quote) from QUOTE_TABLE\\\").show(false)\\r\\n  \",\"kind\":\"spark\",\"dataSourceId\":0,\"componentId\":7,\"componentName\":\"Code 7\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"partitionOptions\":{\"partitionType\":\"\",\"name\":\"\"},\"easyQueryDefJson\":{\"sqlQuery\":\"\"},\"componentId\":6,\"componentName\":\"SQL 6\",\"tableName\":\"SQL_6\",\"category\":\"Processor\",\"componentType\":\"SQL\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"dbTable\":\"import org.apache.spark.sql.functions.udf\\r\\nimport org.apache.spark.sql.functions.col\\r\\nimport org.apache.spark.sql.{Row, SparkSession}\\r\\n\\r\\n  import spark.implicits._\\r\\n  val columns \\u003d Seq(\\\"Seqno\\\",\\\"Quote\\\")\\r\\n  val data \\u003d Seq((\\\"1\\\", \\\"Be the change that you wish to see in the world\\\"),\\r\\n    (\\\"2\\\", \\\"Everyone thinks of changing the world, but no one thinks of changing himself.\\\"),\\r\\n    (\\\"3\\\", \\\"The purpose of our lives is to be happy.\\\")\\r\\n\\r\\n  )\\r\\n  val df \\u003d data.toDF(columns:_*)\\r\\n  df.show(false)\\r\\n\\r\\n  val convertCase \\u003d  (str:String) \\u003d\\u003e {\\r\\n    val arr \\u003d str.split(\\\" \\\")\\r\\n    arr.map(f\\u003d\\u003e  f.substring(0,1).toUpperCase + f.substring(1,f.length)).mkString(\\\" \\\")\\r\\n  }\\r\\n\\r\\n  //Using with DataFrame\\r\\n  val convertUDF \\u003d udf(convertCase)\\r\\n  df.select(col(\\\"Seqno\\\"),\\r\\n    convertUDF(col(\\\"Quote\\\")).as(\\\"Quote\\\") ).show(false)\\r\\n\\r\\n  // Using it on SQL\\r\\n  spark.udf.register(\\\"convertUDF\\\", convertCase)\\r\\n  df.createOrReplaceTempView(\\\"QUOTE_TABLE\\\")\\r\\n  spark.sql(\\\"select Seqno, convertUDF(Quote) from QUOTE_TABLE\\\").show(false)\\r\\n  \"},\"className\":\"com.datagaps.dataflow.models.SQLComponent\",\"isCheckpointEnabled\":\"N\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"import pandas as pd\\r\\n\\r\\ndata \\u003d {\\r\\n  \\\"calories\\\": [420, 380, 390],\\r\\n  \\\"duration\\\": [50, 40, 45]\\r\\n}\\r\\n\\r\\n#load data into a DataFrame object:\\r\\ndf \\u003d pd.DataFrame(data)\\r\\n\\r\\nprint(df) \",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":9,\"componentName\":\"Code 9\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"import java.sql.Timestamp\\r\\nval data \\u003d (0 until 365).map(i \\u003d\\u003e {\\r\\n      val date \\u003d Timestamp.valueOf(\\\"2023-01-01 00:00:00\\\").getTime + i * 24 * 3600 * 1000L\\r\\n      val value \\u003d 100 + i * 0.5 + math.sqrt(i)\\r\\n      (new Timestamp(date), value)\\r\\n    }).toDF(\\\"ds\\\", \\\"y\\\")\\r\\n    data.createOrReplaceTempView(\\\"test_table\\\")\\r\\n    data.show()\",\"kind\":\"spark\",\"dataSourceId\":0,\"componentId\":5,\"componentName\":\"Code 5\",\"tableName\":\"test_table\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"sourceDataFrame\":\"JDBC_3\",\"workschemaTable\":\"obs_issue\",\"componentId\":11,\"componentName\":\"Data Observability 11\",\"tableName\":\"\",\"category\":\"Data Quality\",\"componentType\":\"Data Observability\",\"rank\":0,\"displayRows\":50,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.ObservabilityComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\nimport pandas as pd\\r\\nfrom prophet import Prophet\\r\\n\\r\\ndata\\u003dspark.sql(\\u0027select * from test_table\\u0027)\\r\\n\\r\\ndata_list \\u003d [(row[0], float(row[1])) for row in data.collect()]\\r\\nselected_columns \\u003d pd.DataFrame(data_list, columns\\u003d[\\\"ds\\\", \\\"y\\\"])\\r\\n\\r\\nprint(selected_columns.head())\\r\\nprint(selected_columns.info())\\r\\nselected_columns[\\u0027ds\\u0027]\\u003dpd.to_datetime(selected_columns[\\u0027ds\\u0027])\\r\\n\\r\\n\\r\\nmodel \\u003d Prophet()\\r\\nmodel.fit(selected_columns)\\r\\n\\r\\n# Create a dataframe for future predictions\\r\\nfuture \\u003d model.make_future_dataframe(periods\\u003d30)  # Forecast for 30 days into the future\\r\\n\\r\\n# Make predictions\\r\\nforecast \\u003d model.predict(future)\\r\\n\\r\\n# Print the forecasted values\\r\\nprint(forecast[[\\u0027ds\\u0027, \\u0027yhat\\u0027, \\u0027yhat_lower\\u0027, \\u0027yhat_upper\\u0027]].tail(10))\\r\\nforecast22\\u003dspark.createDataFrame(forecast)\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":12,\"componentName\":\"Code 12\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[5],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"excludeNotification\":\"N\"},{\"partitionOptions\":{\"partitionType\":\"\",\"name\":\"\"},\"easyQueryDefJson\":{\"sqlQuery\":\"\"},\"componentId\":8,\"componentName\":\"SQL 8\",\"tableName\":\"SQL_8\",\"category\":\"Processor\",\"componentType\":\"SQL\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[6],\"options\":{\"dbTable\":\"select currentdate\"},\"className\":\"com.datagaps.dataflow.models.SQLComponent\",\"isCheckpointEnabled\":\"N\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"\\r\\nimport pandas as pd\\r\\nfrom prophet import Prophet\\r\\n\\r\\ndata\\u003dspark.sql(\\u0027select * from test_table\\u0027)\\r\\npandasDF \\u003d data.toPandas()\\r\\n\\r\\n# Initialize and fit the Prophet model\\r\\nmodel \\u003d Prophet()\\r\\nmodel.fit(pandasDF)\\r\\n\\r\\n# Create a dataframe for future predictions\\r\\nfuture \\u003d model.make_future_dataframe(periods\\u003d30)  # Forecast for 30 days into the future\\r\\n\\r\\n# Make predictions\\r\\nforecast \\u003d model.predict(future)\\r\\n\\r\\n# Print the forecasted values\\r\\nprint(forecast[[\\u0027ds\\u0027, \\u0027yhat\\u0027, \\u0027yhat_lower\\u0027, \\u0027yhat_upper\\u0027]].tail(10))\\r\\n\\r\\n#forecast.createOrReplaceTempView(\\u0027python_ts_data\\u0027)\\r\\n\\r\\n#python_df.createOrReplaceTempView(\\\"python_table\\\")\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":10,\"componentName\":\"Code 10\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[5],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":12,"livyOptions":"{\"kind\":\"spark\",\"proxyUser\":\"\",\"jars\":[],\"pyFiles\":[],\"files\":[],\"driverMemory\":\"\",\"driverCores\":0,\"executorMemory\":\"\",\"executorCores\":0,\"numExecutors\":0,\"archives\":[],\"queue\":\"\",\"name\":\"\",\"conf\":{},\"heartbeatTimeoutInSecond\":0}","isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow","workSchemaName":null},"analysis":[{"componentId":11,"name":"Data Observability 11","description":null,"defaultPredictionMethodId":4,"defaultPredictionDefinition":"{\"standardDeviation\":3,\"minDataPoints\":7,\"maxDataPoints\":\"\",\"rollingWindow\":7}","definition":"{\"whereCondition\":\"\",\"groupByColumns\":[{\"category\":\"COUNTRY\",\"columnName\":\"COUNTRY\"}],\"statCollectionDate\":[{\"columnName\":\"C_DATE\",\"format\":\"date\"}]}","enablePrediction":"Y","analysisMeasures":[{"measureName":"DEATHS","columnName":"DEATHS","aggregateId":1,"predictionMethodId":1,"predictionDefinition":"{}"}],"validation":"","enableInlinePrediction":"N","type":"Data Observability","incrementalData":null,"workschemaTable":"obs_issue","workschemaName":"PostgreSQL_49_analysis_ws"}],"datamodels":[],"tagDetails":[],"dataCompares":[]}