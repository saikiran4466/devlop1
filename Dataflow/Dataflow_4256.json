{"dataflow":{"dfKey":"353d2b9c-bca3-4343-938a-5b4077f5b757","name":"Dataflow_4256","tags":null,"description":null,"definition":"{\"name\":\"Dataflow_4256\",\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from pyspark.sql.types import *\\n\\nimport pandas as pd\\n\\nfrom fbprophet import Prophet\\n\\nfrom pyspark.sql.types import StructType,StructField,StringType,TimestampType,ArrayType,DoubleType\\n\\nfrom pyspark.sql.functions import current_date, countDistinct\\n\\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\\n\\nfrom pyspark import StorageLevel \\n\\nfrom datetime import datetime\\n\\nimport ast\\n\\nimport distutils\\n\\nimport distutils.util\\n\\n \\n\\nsql_statement \\u003d \\u0027\\u0027\\u0027\\n\\n  SELECT\\n\\n  \\u0027$[Analysis_ID]\\u0027 as analysis_id,\\n\\n    analysis_measure_id,\\n\\n    category,\\n\\n    first(prediction_meth_def) as json_def,\\n\\n    CAST(run_date as date) as ds,\\n\\n    CAST(SUM(actual_value) as Int) as y\\n\\n  FROM Read_Data_for_Time_series\\n\\n  GROUP BY analysis_measure_id,category, run_date\\n\\n  ORDER BY analysis_measure_id,category, run_date\\n\\n  \\u0027\\u0027\\u0027\\n\\n\\ntime_series_data \\u003d (\\n\\n  spark\\n\\n    .sql( sql_statement )\\n\\n    .repartition(sc.defaultParallelism, [\\u0027analysis_measure_id\\u0027,\\u0027category\\u0027])\\n\\n  ).cache()\\n\\n\\ntime_series_data.createOrReplaceTempView(\\u0027ts_data\\u0027)\\n\\n\\nresult_schema \\u003dStructType([\\n\\n  StructField(\\u0027ds\\u0027,DateType()),\\n\\n  StructField(\\u0027analysis_measure_id\\u0027,StringType()),\\n\\n  StructField(\\u0027category\\u0027,StringType()),\\n\\n   StructField(\\u0027analysis_id\\u0027,StringType()),\\n\\n  StructField(\\u0027y\\u0027,FloatType()),\\n\\n  StructField(\\u0027yhat\\u0027,FloatType()),\\n\\n  StructField(\\u0027yhat_upper\\u0027,FloatType()),\\n\\n  StructField(\\u0027yhat_lower\\u0027,FloatType())\\n\\n  ])\\n\\n\\n\\ndef forecast_time_series( history_pd: pd.DataFrame) -\\u003e pd.DataFrame:\\n\\n\\n  # TRAIN MODEL AS BEFORE\\n\\n  # --------------------------------------\\n\\n  # remove missing values\\n\\n  history_pd\\u003dhistory_pd.dropna()\\n\\n  parameters\\u003dast.literal_eval(history_pd[\\u0027json_def\\u0027].iloc[0])\\n\\n  #print(parameters)\\n\\n  history_pd.drop([\\u0027json_def\\u0027],axis\\u003d1)\\n\\n  # configure the model\\n\\n  model \\u003d Prophet(\\n\\n    interval_width\\u003dfloat(parameters[\\u0027confidenceInterval\\u0027]),\\n\\n    growth\\u003d\\u0027linear\\u0027,\\n\\n    daily_seasonality\\u003dbool(distutils.util.strtobool(parameters[\\u0027seasonality\\u0027][\\u0027daily\\u0027])),\\n\\n    weekly_seasonality\\u003dbool(distutils.util.strtobool(parameters[\\u0027seasonality\\u0027][\\u0027weekly\\u0027])),\\n\\n    yearly_seasonality\\u003dbool(distutils.util.strtobool(parameters[\\u0027seasonality\\u0027][\\u0027yearly\\u0027])),\\n\\n    seasonality_mode\\u003d\\u0027multiplicative\\u0027\\n\\n    )\\n\\n\\n  # train the model\\n\\n  model.fit( history_pd )\\n\\n  # --------------------------------------\\n\\n\\n  # BUILD FORECAST AS BEFORE\\n\\n  # --------------------------------------\\n\\n  # make predictions\\n\\n  future_pd \\u003d model.make_future_dataframe(\\n\\n    periods\\u003d$[future_forecast], \\n\\n    freq\\u003d\\u0027d\\u0027, \\n\\n    include_history\\u003dTrue\\n\\n    )\\n\\n  forecast_pd \\u003d model.predict( future_pd )  \\n\\n  # --------------------------------------\\n\\n\\n  # ASSEMBLE EXPECTED RESULT SET\\n\\n  # --------------------------------------\\n\\n  # get relevant fields from forecast\\n\\n  f_pd \\u003d forecast_pd[ [\\u0027ds\\u0027,\\u0027yhat\\u0027, \\u0027yhat_upper\\u0027, \\u0027yhat_lower\\u0027] ].set_index(\\u0027ds\\u0027)\\n\\n\\n  # get relevant fields from history\\n\\n  h_pd \\u003d history_pd[[\\u0027ds\\u0027,\\u0027analysis_measure_id\\u0027,\\u0027analysis_id\\u0027,\\u0027category\\u0027,\\u0027y\\u0027]].set_index(\\u0027ds\\u0027)\\n\\n\\n  # join history and forecast\\n\\n  results_pd \\u003d f_pd.join( h_pd, how\\u003d\\u0027left\\u0027 )\\n\\n  results_pd.reset_index(level\\u003d0, inplace\\u003dTrue)\\n\\n\\n  # get store \\u0026 item from incoming data set\\n\\n  results_pd[\\u0027analysis_measure_id\\u0027] \\u003d history_pd[\\u0027analysis_measure_id\\u0027].iloc[0]\\n\\n  results_pd[\\u0027category\\u0027] \\u003d history_pd[\\u0027category\\u0027].iloc[0]\\n\\n  results_pd[\\u0027analysis_id\\u0027] \\u003d history_pd[\\u0027analysis_id\\u0027].iloc[0]\\n\\n  # --------------------------------------\\n\\n\\n\\n  return results_pd[ [\\u0027ds\\u0027, \\u0027analysis_measure_id\\u0027,\\u0027analysis_id\\u0027,\\u0027category\\u0027, \\u0027y\\u0027, \\u0027yhat\\u0027, \\u0027yhat_upper\\u0027, \\u0027yhat_lower\\u0027] ]\\n\\n \\n\\nresults \\u003d (\\n\\n  time_series_data\\n\\n    .groupBy(\\u0027analysis_measure_id\\u0027,\\u0027category\\u0027)\\n\\n      .applyInPandas(forecast_time_series, schema\\u003dresult_schema)\\n\\n    .withColumn(\\u0027training_date\\u0027, current_date())\\n\\n    )\\n\\n\\nresults.createOrReplaceTempView(\\u0027small_ts_data\\u0027)\\n\\n \\n\\noutput\\u003dspark.sql(\\\"select ds as PREDICTION_DATE ,analysis_measure_id as ANALYSIS_MEASURE_ID ,category as CATEGORY,analysis_id as ANALYSIS_ID,round(yhat,3) as PREDICTED_VALUE ,round(yhat_upper,3) as UPPER_BOUND,round(yhat_lower,3) as LOWER_BOUND,training_date as RUN_DATE,3 as PREDICTION_METHOD_ID,unix_timestamp(ds) as prediction_run_id  from  small_ts_data\\\")\\n\\n \\n\\noutput.createOrReplaceTempView(\\u0027output_time_series_data\\u0027)\\n\\noutput.cache()\\n\\noutput.count()\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":2,\"componentName\":\"Code 2\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"partitionOptions\":{\"partitionType\":\"\",\"name\":\"\"},\"enableTrim\":\"N\",\"easyQueryDefJson\":{\"sqlQuery\":\"\"},\"componentId\":3,\"componentName\":\"JDBC 3\",\"tableName\":\"JDBC_3\",\"category\":\"Source\",\"componentType\":\"JDBC\",\"rank\":0,\"dataSourceName\":\"Oracle_new2\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"dbTable\":\"select * from da_covid1copy1\"},\"className\":\"com.datagaps.dataflow.models.JDBCComponent\",\"isCheckpointEnabled\":\"N\",\"dataSourceLogicalName\":\"\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"sourceDataFrame\":\"JDBC_3\",\"workschemaTable\":\"Data_Observability_4_4974\",\"componentId\":4,\"componentName\":\"Data Observability 4\",\"tableName\":\"\",\"category\":\"Data Quality\",\"componentType\":\"Data Observability\",\"rank\":0,\"displayRows\":50,\"dependencies\":[3],\"className\":\"com.datagaps.dataflow.models.ObservabilityComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":4,"livyOptions":"{\"kind\":\"spark\",\"proxyUser\":\"\",\"jars\":[],\"pyFiles\":[],\"files\":[],\"driverMemory\":\"\",\"driverCores\":0,\"executorMemory\":\"\",\"executorCores\":0,\"numExecutors\":0,\"archives\":[],\"queue\":\"\",\"name\":\"\",\"conf\":{},\"heartbeatTimeoutInSecond\":0}","isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow","workSchemaName":null},"analysis":[],"datamodels":[],"tagDetails":[],"dataCompares":[]}