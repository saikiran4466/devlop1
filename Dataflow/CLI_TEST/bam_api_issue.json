{"dataflow":{"dfKey":"3dc34926-cc93-479d-ad57-84b9cc3b3ed3","name":"bam_api_issue","tags":null,"description":null,"definition":"{\"name\":\"bam_api_issue\",\"version\":0,\"livyServerId\":0,\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"import requests\\r\\nfrom datagaps_utilities import json_to_rdb\\r\\n\\r\\nurl \\u003d \\\"https://raw.githubusercontent.com/json-iterator/test-data/master/large-file.json\\\"\\r\\n\\r\\npayload\\u003d{}\\r\\nheaders \\u003d {}\\r\\n\\r\\nresponse \\u003d requests.request(\\\"GET\\\", url, headers\\u003dheaders, data\\u003dpayload)\\r\\n\\r\\n\\r\\n# json_to_rdb(session_object\\u003dspark ,data\\u003dresponse.json(),primary_table_name\\u003d\\\"bam\\\",use_short_names\\u003dFalse,parse_all_strings\\u003dFalse,component_name\\u003d\\\"Code 1\\\",dataflow_run_id\\u003d$[dataflow_run_id],dataflow_id\\u003d\\\"$[dataflow_id]\\\")\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":1,\"componentName\":\"Code 1\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"import requests\\r\\n\\r\\nurl \\u003d \\\"https://data.wa.gov/api/views/f6w7-q2d2/rows.json?accessType\\u003dDOWNLOAD\\\"\\r\\n\\r\\npayload\\u003d{}\\r\\nheaders \\u003d {}\\r\\n\\r\\nresponse \\u003d requests.request(\\\"GET\\\", url, headers\\u003dheaders, data\\u003dpayload)\\r\\n\\r\\njson_to_rdb(session_object\\u003dspark ,data\\u003dresponse.json(),primary_table_name\\u003d\\\"large\\\",use_short_names\\u003dFalse,parse_all_strings\\u003dFalse,component_name\\u003d\\\"Code 2\\\",dataflow_run_id\\u003d$[dataflow_run_id],dataflow_id\\u003d\\\"$[dataflow_id]\\\")\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":2,\"componentName\":\"Code 2\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"import requests\\r\\nfrom datagaps_utilities import json_to_rdb\\r\\n\\r\\nurl \\u003d \\\"https://raw.githubusercontent.com/zemirco/sf-city-lots-json/master/citylots.json\\\"\\r\\n\\r\\npayload\\u003d{}\\r\\nheaders \\u003d {}\\r\\n\\r\\nresponse \\u003d requests.request(\\\"GET\\\", url, headers\\u003dheaders, data\\u003dpayload)\\r\\n\\r\\njson_to_rdb(session_object\\u003dspark ,data\\u003dresponse.json(),primary_table_name\\u003d\\\"large\\\",use_short_names\\u003dFalse,parse_all_strings\\u003dFalse,component_name\\u003d\\\"Code 3\\\",dataflow_run_id\\u003d$[dataflow_run_id],dataflow_id\\u003d\\\"$[dataflow_id]\\\")\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":3,\"componentName\":\"Code 3\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"import json\\r\\nimport os\\r\\nimport re\\r\\nfrom pyspark import StorageLevel\\r\\n\\r\\n# from .inner_functions import split_as_relational_data, pprint_tables, prepare_relation_dict, \\\\\\r\\n#     create_datasets_and_trigger_scala_methods\\r\\n\\r\\nimport json\\r\\nimport re\\r\\nfrom collections import defaultdict\\r\\n# import pandas as pd\\r\\nimport requests\\r\\n# from requests_ntlm import HttpNtlmAuth\\r\\n\\r\\n# from datagaps_utilities.config import aes_key\\r\\n\\r\\n\\r\\ndef split_as_relational_data(input_json\\u003dNone, primary_table_name\\u003dNone, use_short_names\\u003dFalse, parse_all_strings\\u003dFalse):\\r\\n    \\\"\\\"\\\"\\r\\n    function to convert list/dict into relational data\\r\\n    :param parse_all_strings: it will apply json parse on all strings\\r\\n    :param input_json: json data as string or python object, (data or file_path any one is mandatory)\\r\\n    :param primary_table_name: optional, name of primary table, if not specified it will take file name or \\\"datagaps\\\"\\r\\n    :param use_short_names: its boolean, if its True then replace large table names with short names\\r\\n    :return: dict, it has table names as keys and data in the form of list of dicts as values\\r\\n    \\\"\\\"\\\"\\r\\n\\r\\n    final_dict \\u003d defaultdict(list)\\r\\n    parent_tables \\u003d defaultdict(set)\\r\\n    short_names \\u003d {}\\r\\n\\r\\n    def make_table(table_name, data):\\r\\n        u_id \\u003d len(final_dict[table_name]) + 1\\r\\n        d \\u003d {\\\"dg_uid\\\": u_id}\\r\\n        if use_short_names:\\r\\n            if short_names.get(table_name):\\r\\n                short_table_name \\u003d short_names[table_name]\\r\\n            else:\\r\\n                short_table_name \\u003d f\\\"t{len(short_names)}_{table_name.split(\\u0027_\\u0027)[-1]}\\\"\\r\\n                short_names[table_name] \\u003d short_table_name\\r\\n        else:\\r\\n            short_table_name \\u003d table_name\\r\\n        for k, v in data.items():\\r\\n            if parse_all_strings and v and type(v) \\u003d\\u003d str and k !\\u003d \\\"value\\\" and v.strip()[0] in [\\\"{\\\", \\\"[\\\"]:\\r\\n                try:\\r\\n                    v \\u003d json.loads(v)\\r\\n                except:\\r\\n                    pass\\r\\n            if type(v) \\u003d\\u003d dict:\\r\\n                make_table(f\\\"{table_name}_{k}\\\", {f\\\"{short_table_name}_id\\\": u_id, **v})\\r\\n                parent_tables[table_name].add(f\\\"{table_name}_{k}\\\")\\r\\n            elif type(v) \\u003d\\u003d list:\\r\\n                send_rows(f\\\"{table_name}_{k}\\\", v, (f\\\"{short_table_name}_id\\\", u_id))\\r\\n                parent_tables[table_name].add(f\\\"{table_name}_{k}\\\")\\r\\n            else:\\r\\n                d[k] \\u003d v\\r\\n        final_dict[table_name].append(d)\\r\\n\\r\\n    def send_rows(table_name, data_list, fk\\u003dNone):\\r\\n        if fk:\\r\\n            if not data_list:\\r\\n                make_table(table_name, {fk[0]: fk[1]})\\r\\n            for i in data_list:\\r\\n                if parse_all_strings and i and type(i) \\u003d\\u003d str and i.strip()[0] in [\\\"{\\\", \\\"[\\\"]:\\r\\n                    try:\\r\\n                        i \\u003d json.loads(i)\\r\\n                    except:\\r\\n                        pass\\r\\n                if type(i) \\u003d\\u003d dict:\\r\\n                    make_table(table_name, {**i, fk[0]: fk[1]})\\r\\n                else:\\r\\n                    make_table(table_name, {\\\"value\\\": i, fk[0]: fk[1]})\\r\\n        else:\\r\\n            if not data_list:\\r\\n                make_table(table_name, {})\\r\\n            for i in data_list:\\r\\n                if parse_all_strings and i and type(i) \\u003d\\u003d str and i.strip()[0] in [\\\"{\\\", \\\"[\\\"]:\\r\\n                    try:\\r\\n                        i \\u003d json.loads(i)\\r\\n                    except:\\r\\n                        pass\\r\\n                if type(i) \\u003d\\u003d dict:\\r\\n                    make_table(table_name, i)\\r\\n                else:\\r\\n                    make_table(table_name, {\\\"value\\\": i})\\r\\n\\r\\n    if type(input_json) \\u003d\\u003d list:\\r\\n        send_rows(primary_table_name, input_json)\\r\\n    else:\\r\\n        make_table(primary_table_name, input_json)\\r\\n\\r\\n    final_dict \\u003d dict(final_dict)\\r\\n    parent_tables \\u003d dict(parent_tables)\\r\\n\\r\\n    return {\\\"final_dict\\\": final_dict, \\\"parent_tables\\\": parent_tables, \\\"short_names\\\": short_names}\\r\\n\\r\\n\\r\\ndef pprint_tables(primary_table_name, parent_tables, short_names\\u003dNone):\\r\\n    def print_table_relation(table_name, count):\\r\\n        print(\\\"\\\\t\\\" * 2 * count, \\\"--\\u003e\\\", table_name)\\r\\n        for i in parent_tables.get(table_name, []):\\r\\n            print_table_relation(i, count + 1)\\r\\n\\r\\n    print_table_relation(primary_table_name, 0)\\r\\n    print(\\\"\\\\n\\\\n\\\")\\r\\n    if short_names:\\r\\n        print(\\\"{:\\u003c20} {}\\\".format(\\\"short name\\\", \\\"detailed name\\\"))\\r\\n        print(\\\"{:\\u003c20} {}\\\".format(\\\"--------------\\\", \\\"-----------------------\\\"))\\r\\n        for k, v in short_names.items():\\r\\n            print(\\\"{:\\u003c20} {}\\\".format(v, k))\\r\\n\\r\\n\\r\\ndef prepare_relation_dict(primary_table_name, parent_tables):\\r\\n    relation_dict \\u003d {primary_table_name: {}}\\r\\n\\r\\n    def update_relation_dict(table_name, table_dict):\\r\\n        for i in parent_tables.get(table_name, []):\\r\\n            table_dict[i] \\u003d {}\\r\\n            update_relation_dict(i, table_dict[i])\\r\\n\\r\\n    update_relation_dict(primary_table_name, relation_dict[primary_table_name])\\r\\n    return relation_dict\\r\\n\\r\\n\\r\\n\\r\\ndef run_pre_script(pre_script, spark):\\r\\n    pre_script_variables \\u003d {}\\r\\n    exec(pre_script)\\r\\n    return pre_script_variables\\r\\n\\r\\n\\r\\ndef run_pre_script_and_trigger_api(api_details, session_object):\\r\\n    def password_vault(key):\\r\\n        key \\u003d str(key).strip()\\r\\n        if key.startswith(\\\"#[\\\") and key.endswith(\\\"]\\\"):\\r\\n            return session_object.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getPasswordFromWallet(key[2:-1])\\r\\n        else:\\r\\n            return key\\r\\n    api_details \\u003d json.loads(api_details)\\r\\n    pre_script \\u003d api_details.pop(\\\"preRequestScript\\\")\\r\\n    pre_script_variables \\u003d run_pre_script(pre_script, session_object)\\r\\n    print(pre_script_variables)\\r\\n\\r\\n    url \\u003d api_details[\\\"url\\\"]\\r\\n    method \\u003d api_details[\\\"requestType\\\"]\\r\\n    payload \\u003d api_details[\\\"body\\\"]\\r\\n    headers \\u003d api_details[\\\"headers\\\"]\\r\\n    headers \\u003d {password_vault(i[\\\"key\\\"]): password_vault(i[\\\"value\\\"]) for i in headers}\\r\\n    params \\u003d api_details[\\\"params\\\"]\\r\\n    params \\u003d {password_vault(i[\\\"key\\\"]): password_vault(i[\\\"value\\\"]) for i in params}\\r\\n    authorization \\u003d api_details[\\\"authorization\\\"]\\r\\n    auth \\u003d None\\r\\n\\r\\n    if authorization[\\\"type\\\"] \\u003d\\u003d \\\"Basic Auth\\\":\\r\\n        try:\\r\\n            decrypted_password \\u003d session_object.sparkContext._jvm.com.datagaps.core.engine.security.DecryptUtils.pwdDecrypt(\\r\\n                aes_key, authorization[\\\"password\\\"])\\r\\n        except:\\r\\n            decrypted_password \\u003d authorization[\\\"password\\\"]\\r\\n        auth \\u003d (authorization[\\\"userName\\\"], decrypted_password)\\r\\n    elif authorization[\\\"type\\\"] \\u003d\\u003d \\\"NTLM Authentication\\\":\\r\\n        try:\\r\\n            decrypted_password \\u003d session_object.sparkContext._jvm.com.datagaps.core.engine.security.DecryptUtils.pwdDecrypt(\\r\\n                aes_key, authorization[\\\"password\\\"])\\r\\n        except:\\r\\n            decrypted_password \\u003d authorization[\\\"password\\\"]\\r\\n        if authorization[\\\"domain\\\"]:\\r\\n            auth \\u003d HttpNtlmAuth(f\\u0027{authorization[\\\"domain\\\"]}\\\\\\\\{authorization[\\\"userName\\\"]}\\u0027, decrypted_password)\\r\\n        else:\\r\\n            auth \\u003d HttpNtlmAuth(authorization[\\\"userName\\\"], decrypted_password)\\r\\n    elif authorization[\\\"type\\\"] \\u003d\\u003d \\\"API Key\\\":\\r\\n        if authorization[\\\"addTo\\\"] \\u003d\\u003d \\\"Header\\\":\\r\\n            headers[authorization[\\\"key\\\"]] \\u003d authorization[\\\"value\\\"]\\r\\n        else:\\r\\n            params[authorization[\\\"key\\\"]] \\u003d authorization[\\\"value\\\"]\\r\\n    elif authorization[\\\"type\\\"] \\u003d\\u003d \\\"Bearer Token\\\":\\r\\n        if authorization[\\\"isVariable\\\"] \\u003d\\u003d \\\"Y\\\":\\r\\n            headers[\\\"Authorization\\\"] \\u003d f\\\"Bearer {pre_script_variables[authorization[\\u0027token\\u0027]]}\\\"\\r\\n        else:\\r\\n            headers[\\\"Authorization\\\"] \\u003d f\\\"Bearer {authorization[\\u0027token\\u0027]}\\\"\\r\\n\\r\\n    try:\\r\\n        response \\u003d requests.request(method, url, params\\u003dparams, headers\\u003dheaders, data\\u003dpayload, auth\\u003dauth)\\r\\n    except requests.exceptions.SSLError:\\r\\n        response \\u003d requests.request(method, url, params\\u003dparams, headers\\u003dheaders, data\\u003dpayload, auth\\u003dauth, verify\\u003dFalse)\\r\\n    return response\\r\\n\\r\\n\\r\\ndef create_meta_data(session_object, final_dict, short_names\\u003dNone):\\r\\n    meta_data_of_all_tables \\u003d []\\r\\n    for k, v in final_dict.items():\\r\\n        # df \\u003d pd.DataFrame(v)\\r\\n        # df1 \\u003d session_object.createDataFrame(df)\\r\\n        df1 \\u003d session_object.read.json(session_object.sparkContext.parallelize(v).map(lambda x: json.dumps(x)))\\r\\n\\r\\n        df1 \\u003d df1.toDF(*[re.sub(\\u0027[^0-9A-Za-z_]\\u0027, \\u0027_\\u0027, c) for c in df1.columns])\\r\\n        if short_names:\\r\\n            k \\u003d short_names[k]\\r\\n        view_name \\u003d re.sub(\\\"[^0-9A-Za-z_]\\\", \\\"_\\\", k)\\r\\n        schema \\u003d [json.loads(i.json()) for i in df1.schema.fields]\\r\\n        meta_data \\u003d [{\\\"table_name\\\": view_name, \\\"column_name\\\": i[\\\"name\\\"], \\\"data_type\\\": i[\\\"type\\\"]} for i in schema]\\r\\n        meta_data_of_all_tables.extend(meta_data)\\r\\n    return meta_data_of_all_tables\\r\\n\\r\\n\\r\\ndef create_datasets_and_trigger_scala_methods(relation_dict, parent, final_dict, session_object,\\r\\n                                              component_name, dataflow_run_id, dataflow_id, no_prints\\u003dFalse):\\r\\n    all_dataset_names \\u003d []\\r\\n    for view_name, child_tables in relation_dict.items():\\r\\n        rdd \\u003d session_object.sparkContext.parallelize(final_dict[view_name])\\r\\n        no_of_partitions \\u003d rdd.getNumPartitions()\\r\\n        sdf \\u003d session_object.read.json(rdd.map(lambda x: json.dumps(x)))\\r\\n        sdf \\u003d sdf.toDF(*[re.sub(\\u0027[^0-9A-Za-z_]\\u0027, \\u0027_\\u0027, c) for c in sdf.columns])\\r\\n        #sdf.cache()\\r\\n        #sdf.persist()\\r\\n        #sdf.persist(StorageLevel.MEMORY_AND_DISK)\\r\\n        #sdf.persist(StorageLevel.DISK_ONLY)\\r\\n        view_name \\u003d re.sub(\\u0027[^0-9A-Za-z_]\\u0027, \\u0027_\\u0027, view_name)\\r\\n        sdf.createOrReplaceTempView(view_name)\\r\\n        schema \\u003d [json.loads(i.json()) for i in sdf.schema.fields]\\r\\n        schema \\u003d json.dumps({\\\"type\\\": \\\"struct\\\", \\\"fields\\\": schema})\\r\\n        sample_output \\u003d json.dumps([json.loads(i) for i in sdf.limit(100).toJSON().collect()])\\r\\n        no_of_rows \\u003d sdf.count()\\r\\n        if parent is None:\\r\\n            session_object.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDatasetInfo(view_name,\\r\\n                                                                                                      schema,\\r\\n                                                                                                      no_of_rows,\\r\\n                                                                                                      no_of_partitions,\\r\\n                                                                                                      dataflow_run_id,\\r\\n                                                                                                      sample_output,\\r\\n                                                                                                      component_name)\\r\\n        else:\\r\\n            relations \\u003d json.dumps([{\\\"parentTableName\\\": parent, \\\"parentColumnName\\\": \\\"dg_uid\\\",\\r\\n                                     \\\"childTableName\\\": view_name,\\r\\n                                     \\\"childColumnName\\\": f\\\"{parent}_id\\\", \\\"relationType\\\": \\\"one-to-many\\\"}])\\r\\n\\r\\n            session_object.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDatasetWithRelations(\\r\\n                view_name,\\r\\n                schema,\\r\\n                no_of_rows,\\r\\n                no_of_partitions,\\r\\n                sample_output,\\r\\n                component_name,\\r\\n                dataflow_run_id,\\r\\n                relations,\\r\\n                dataflow_id\\r\\n            )\\r\\n\\r\\n            # print(relations)\\r\\n        if not no_prints:\\r\\n            print(view_name)\\r\\n            sdf.show(5)\\r\\n            print(\\\"\\\\n\\\\n\\\")\\r\\n        # print(f\\\"name\\u003d{view_name} parent\\u003d{parent}\\\")\\r\\n        all_dataset_names.append(view_name)\\r\\n        child_dataset_names \\u003d create_datasets_and_trigger_scala_methods(child_tables, view_name, final_dict,\\r\\n                                                                        session_object,\\r\\n                                                                        component_name, dataflow_run_id,\\r\\n                                                                        dataflow_id, no_prints\\u003dno_prints)\\r\\n        all_dataset_names.extend(child_dataset_names)\\r\\n    return all_dataset_names\\r\\n\\r\\ndef json_to_rdb(session_object, component_name, dataflow_run_id, dataflow_id, file_path\\u003dNone, data\\u003dNone,\\r\\n                primary_table_name\\u003dNone, use_short_names\\u003dFalse,\\r\\n                parse_all_strings\\u003dFalse):\\r\\n    \\\"\\\"\\\"\\r\\n    function to convert json data into relational spark datasets\\r\\n    :param dataflow_id: dataflow id by using datagaps inbuilt parameter $[dataflow_id]\\r\\n    :param dataflow_run_id: dataflow run id by using datagaps inbuilt parameter $[dataflow_run_id]\\r\\n    :param component_name: name of code component or plugin component\\r\\n    :param parse_all_strings: it will apply json parse on all strings\\r\\n    :param session_object: mandatory, spark session object from code component\\r\\n    :param file_path: json file path, (data or file_path any one is mandatory)\\r\\n    :param data: json data as string or python object, (data or file_path any one is mandatory)\\r\\n    :param primary_table_name: optional, name of primary table, if not specified it will take file name or \\\"datagaps\\\"\\r\\n    :param use_short_names: its boolean, if its True then replace large table names with short names\\r\\n    :return: None, just prints table relation and top 5 records in each table\\r\\n    \\\"\\\"\\\"\\r\\n    if file_path:\\r\\n        with open(file_path, encoding\\u003d\\\"utf-8\\\") as f:\\r\\n            data \\u003d f.read()\\r\\n        if not primary_table_name:\\r\\n            primary_table_name \\u003d os.path.basename(file_path).split(\\\".\\\")[0]\\r\\n    else:\\r\\n        if not primary_table_name:\\r\\n            primary_table_name \\u003d \\\"datagaps\\\"\\r\\n\\r\\n    if type(data) in (list, dict):\\r\\n        input_json \\u003d data\\r\\n    else:\\r\\n        try:\\r\\n            input_json \\u003d json.loads(data)\\r\\n        except:\\r\\n            try:\\r\\n                input_json \\u003d json.loads(\\\"[\\\" + data.strip().replace(\\\"\\\\n\\\", \\\",\\\") + \\\"]\\\")\\r\\n            except:\\r\\n                print(\\\"Unable to read Json data\\\")\\r\\n                raise\\r\\n\\r\\n    relational_data \\u003d split_as_relational_data(input_json\\u003dinput_json, primary_table_name\\u003dprimary_table_name,\\r\\n                                               use_short_names\\u003duse_short_names,\\r\\n                                               parse_all_strings\\u003dparse_all_strings)\\r\\n\\r\\n    final_dict \\u003d relational_data[\\\"final_dict\\\"]\\r\\n    parent_tables \\u003d relational_data[\\\"parent_tables\\\"]\\r\\n    short_names \\u003d relational_data[\\\"short_names\\\"]\\r\\n    # create_spark_datasets(session_object, final_dict, short_names)\\r\\n    if short_names:\\r\\n        primary_table_name \\u003d short_names[primary_table_name]\\r\\n        parent_tables \\u003d {short_names[k]: [short_names[i] for i in v] for k, v in parent_tables.items()}\\r\\n        final_dict \\u003d {short_names[k]: v for k, v in final_dict.items()}\\r\\n\\r\\n    primary_table_name \\u003d re.sub(\\\"[^0-9A-Za-z_]\\\", \\\"_\\\", primary_table_name)\\r\\n    parent_tables \\u003d {re.sub(\\\"[^0-9A-Za-z_]\\\", \\\"_\\\", k): [re.sub(\\\"[^0-9A-Za-z_]\\\", \\\"_\\\", i) for i in v] for k, v in\\r\\n                     parent_tables.items()}\\r\\n    final_dict \\u003d {re.sub(\\\"[^0-9A-Za-z_]\\\", \\\"_\\\", k): v for k, v in final_dict.items()}\\r\\n    relation_dict \\u003d prepare_relation_dict(primary_table_name, parent_tables)\\r\\n\\r\\n    dataset_names \\u003d create_datasets_and_trigger_scala_methods(relation_dict, None, final_dict, session_object,\\r\\n                                                              component_name, dataflow_run_id, dataflow_id)\\r\\n    session_object.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.deleteDatasetInfo(\\r\\n        json.dumps(dataset_names),\\r\\n        dataflow_run_id,\\r\\n        component_name)\\r\\n    pprint_tables(primary_table_name, parent_tables, short_names)\\r\\n\\r\\nimport requests\\r\\n\\r\\nurl \\u003d \\\"https://raw.githubusercontent.com/zemirco/sf-city-lots-json/master/citylots.json\\\"\\r\\n\\r\\npayload\\u003d{}\\r\\nheaders \\u003d {}\\r\\n\\r\\nresponse \\u003d requests.request(\\\"GET\\\", url, headers\\u003dheaders, data\\u003dpayload)\\r\\n\\r\\njson_to_rdb(session_object\\u003dspark ,data\\u003dresponse.json(),primary_table_name\\u003d\\\"large1\\\",use_short_names\\u003dFalse,parse_all_strings\\u003dFalse,component_name\\u003d\\\"Code 5\\\",dataflow_run_id\\u003d$[dataflow_run_id],dataflow_id\\u003d\\\"$[dataflow_id]\\\")\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":5,\"componentName\":\"Code 5\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"partitionOptions\":{\"partitionType\":\"\",\"name\":\"\"},\"easyQueryDefJson\":{\"sqlQuery\":\"\"},\"componentId\":6,\"componentName\":\"SQL 6\",\"tableName\":\"SQL_6\",\"category\":\"Processor\",\"componentType\":\"SQL\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"dbTable\":\"select BLKLOT,BLOCK_NUM,FROM_ST,LOT_NUM,MAPBLKLOT,ODD_EVEN,STREET,ST_TYPE,TO_ST,dg_uid,large1_features_id from large1_features_properties\"},\"className\":\"com.datagaps.dataflow.models.SQLComponent\",\"isCheckpointEnabled\":\"N\",\"executionOption\":\"\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":6,"livyOptions":"{\"kind\":\"spark\",\"proxyUser\":\"\",\"jars\":[],\"pyFiles\":[],\"files\":[],\"driverMemory\":\"\",\"driverCores\":0,\"executorMemory\":\"\",\"executorCores\":0,\"numExecutors\":0,\"archives\":[],\"queue\":\"\",\"name\":\"\",\"conf\":{},\"heartbeatTimeoutInSecond\":0}","isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow/CLI_TEST","workSchemaName":null},"analysis":[],"datamodels":[],"tagDetails":[],"dataCompares":[]}