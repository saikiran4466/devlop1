{"dataflow":{"dfKey":"54e068cb-be03-4ece-b29a-073e9afc41e3","name":"adls_excel_plugin_Copy","tags":null,"description":" ","definition":"{\"name\":\"adls_excel_plugin_Copy\",\"description\":\" \",\"version\":0,\"livyServerId\":0,\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"pluginId\":\"ed8476c8-dae6-4863-a0e6-ff76506bd6fc\",\"pluginName\":\"ADLS excel read single and multiple sheets\",\"pluginOptions\":{\"parameters\":[{\"parameterId\":\"0eb9055c-baa4-4a30-8184-a34dccfef01b\",\"fieldName\":\"file_name\",\"displayName\":\"File Name\",\"value\":\"sample1.xls\",\"dataType\":\"String\"},{\"parameterId\":\"c7e358d4-77d5-4941-b1ea-c63eada34ad7\",\"fieldName\":\"sheet_names\",\"displayName\":\"Sheet names\",\"value\":\"None\",\"dataType\":\"text\"}],\"inputDatasets\":[],\"outputDatasets\":[],\"dataSources\":[{\"paramId\":\"69d29094-0bf9-4a3b-8585-6550a5c65da5\",\"fieldName\":\"data_source\",\"displayName\":\"Select Excel Data Source\",\"value\":\"1917\"}]},\"componentId\":5,\"componentName\":\"Plugin 5\",\"tableName\":\"Plugin_5\",\"category\":\"Processor\",\"componentType\":\"Plugin\",\"rank\":0,\"displayRows\":0,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.PluginComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"from datagaps_utilities import config\\n\\nprint(config.app_version)\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":6,\"componentName\":\"Code 6\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"pluginId\":\"ed8476c8-dae6-4863-a0e6-ff76506bd6fc\",\"pluginName\":\"ADLS excel read single and multiple sheets\",\"pluginOptions\":{\"parameters\":[{\"parameterId\":\"0eb9055c-baa4-4a30-8184-a34dccfef01b\",\"fieldName\":\"file_name\",\"displayName\":\"File Name\",\"value\":\"apachedrill (1).xlsx\",\"dataType\":\"String\"},{\"parameterId\":\"087037a3-3beb-4664-8b03-fb5b8cf5733d\",\"fieldName\":\"component_name\",\"displayName\":\"Component Name\",\"value\":\"Plugin 7\",\"dataType\":\"String\"},{\"parameterId\":\"c7e358d4-77d5-4941-b1ea-c63eada34ad7\",\"fieldName\":\"sheet_names\",\"displayName\":\"Sheet names\",\"value\":\"None\",\"dataType\":\"text\"}],\"inputDatasets\":[],\"outputDatasets\":[],\"dataSources\":[{\"paramId\":\"69d29094-0bf9-4a3b-8585-6550a5c65da5\",\"fieldName\":\"data_source\",\"displayName\":\"Select Excel Data Source\",\"value\":\"2068\"}]},\"componentId\":7,\"componentName\":\"Plugin 7\",\"tableName\":\"Plugin_7\",\"category\":\"Processor\",\"componentType\":\"Plugin\",\"rank\":0,\"displayRows\":0,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.PluginComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\n\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\nimport pandas as pd\\r\\nfrom azure.storage.filedatalake import DataLakeServiceClient\\r\\nimport re\\r\\nimport os\\r\\nimport json\\r\\nfrom collections import defaultdict\\r\\nimport traceback\\r\\nfrom pyspark.sql.types import *\\r\\n\\r\\ntry:\\r\\n    sheet_names \\u003d None\\r\\n    if sheet_names is None or type(sheet_names) \\u003d\\u003d list:\\r\\n        if type(sheet_names) \\u003d\\u003d list:\\r\\n            if len(sheet_names) \\u003d\\u003d 0:\\r\\n                raise Exception(\\\"sheet names should not be empty list\\\")\\r\\n    else:\\r\\n        raise Exception(\\\"sheet names should be list or None\\\")\\r\\n    \\r\\n    java_object \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getConnectionParams(\\\"EXCEL_adls_upendar_2\\\", \\\"read\\\")\\r\\n    \\r\\n    data_source_details_list \\u003d [java_object.toList().apply(i) for i in range(java_object.size())]\\r\\n    data_source_details \\u003d {i._1(): i._2() for i in data_source_details_list}\\r\\n    \\r\\n    storage_account_name \\u003d data_source_details[\\\"storage-account\\\"]\\r\\n    storage_account_key \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getDecryptedPassword(\\r\\n        data_source_details[\\\"access-key\\\"])\\r\\n    file_system_name \\u003d data_source_details[\\\"container\\\"]\\r\\n    file_full_path \\u003d os.path.join(data_source_details[\\\"path\\\"], \\\"apachedrill (1).xlsx\\\")\\r\\n    header \\u003d 0 if data_source_details.get(\\\"useHeader\\\") \\u003d\\u003d \\\"true\\\" else None\\r\\n    \\r\\n    try:\\r\\n        service_client \\u003d DataLakeServiceClient(account_url\\u003d\\\"{}://{}.dfs.core.windows.net\\\".format(\\r\\n            \\\"https\\\", storage_account_name), credential\\u003dstorage_account_key)\\r\\n    except Exception as e:\\r\\n        print(e)\\r\\n    \\r\\n    \\r\\n    def read_file_from_directory(file_path):\\r\\n        try:\\r\\n            file_system_client \\u003d service_client.get_file_system_client(file_system\\u003dfile_system_name)\\r\\n    \\r\\n            file_client \\u003d file_system_client.get_file_client(file_path)\\r\\n            download \\u003d file_client.download_file()\\r\\n            downloaded_bytes \\u003d download.readall()\\r\\n            return downloaded_bytes\\r\\n        except Exception as ex:\\r\\n            print(ex)\\r\\n    \\r\\n    \\r\\n    downloaded_file \\u003d read_file_from_directory(file_full_path)\\r\\n    \\r\\n    df_dict \\u003d pd.read_excel(downloaded_file, sheet_name\\u003dsheet_names, header\\u003dheader)\\r\\n    print()\\r\\n    ls \\u003d []\\r\\n    component_name \\u003d \\\"Plugin 7\\\"\\r\\n    for sheet_name, dataframe in df_dict.items():\\r\\n        if dataframe.empty:\\r\\n            emp_rdd \\u003d spark.sparkContext.emptyRDD()\\r\\n            columns \\u003d StructType([])\\r\\n            spark_dataframe \\u003d spark.createDataFrame(data \\u003d emp_rdd,\\r\\n                                         schema \\u003d columns)\\r\\n        else:\\r\\n            spark_dataframe \\u003d spark.createDataFrame(dataframe.astype(\\u0027str\\u0027))\\r\\n        dataset_name \\u003d re.sub(\\\"[^0-9A-Za-z_]\\\", \\\"_\\\", f\\\"{component_name}_{sheet_name}\\\")\\r\\n    \\r\\n        column_map1 \\u003d defaultdict(list)\\r\\n        column_map2 \\u003d {}\\r\\n        for column in spark_dataframe.columns:\\r\\n            m_column \\u003d re.sub(\\u0027[^0-9A-Za-z_]\\u0027, \\u0027_\\u0027, column)\\r\\n            column_map1[m_column].append(column)\\r\\n            column_map2[column] \\u003d m_column\\r\\n        for n_column, o_columns in column_map1.items():\\r\\n            if len(o_columns) \\u003e 1:\\r\\n                for num, o_col in enumerate(o_columns):\\r\\n                    column_map2[o_col] \\u003d f\\\"{n_column}_{num + 1}\\\"\\r\\n        spark_dataframe \\u003d spark_dataframe.toDF(*[column_map2[c] for c in spark_dataframe.columns])\\r\\n    \\r\\n        spark_dataframe.createOrReplaceTempView(dataset_name)\\r\\n        print(dataset_name)\\r\\n        ls.append(dataset_name)\\r\\n        spark_dataframe.show()\\r\\n        print()\\r\\n        spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(spark_dataframe._jdf, dataset_name, \\\"Plugin 7\\\", 22367)\\r\\n    \\r\\n    spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.deleteDatasetInfo(json.dumps(ls),22367,\\\"Plugin 7\\\")\\r\\nexcept:\\r\\n    print(traceback.format_exc())\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":8,\"componentName\":\"Code 8\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":8,"livyOptions":"{\"proxyUser\":\"\",\"jars\":[],\"pyFiles\":[],\"files\":[],\"driverMemory\":\"\",\"driverCores\":0,\"executorMemory\":\"\",\"executorCores\":0,\"numExecutors\":0,\"archives\":[],\"queue\":\"\",\"name\":\"\",\"conf\":{},\"heartbeatTimeoutInSecond\":0,\"kind\":\"spark\"}","isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow/CLI_TEST","workSchemaName":null},"analysis":[],"datamodels":[],"tagDetails":[],"dataCompares":[]}