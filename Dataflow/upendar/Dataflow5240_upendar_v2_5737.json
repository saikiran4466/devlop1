{"dataflow":{"dfKey":"8c6bdf5e-489f-424a-8264-3649f39e79fb","name":"Dataflow5240_upendar_v2_5737","tags":null,"description":"Â southern.co client task excel plugin and email","definition":"{\"name\":\"Dataflow5240_upendar_v2_5737\",\"description\":\"Â southern.co client task excel plugin and email\",\"version\":0,\"livyServerId\":0,\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\n\\nimport pandas as pd\\nd \\u003d [\\n    {\\\"c1\\\":\\\"ghg\\\", \\\"c2\\\": 23, \\\"c3\\\":3.8},\\n    {\\\"c1\\\":\\\"ffg\\\", \\\"c2\\\": 65, \\\"c3\\\":5.89},\\n    {\\\"c1\\\":\\\"yty\\\", \\\"c2\\\": 33, \\\"c3\\\":6.8},\\n    {\\\"c1\\\":\\\"rtre\\\", \\\"c2\\\": 23, \\\"c3\\\":5.78},\\n    {\\\"c1\\\":\\\"ryr\\\", \\\"c2\\\": 45, \\\"c3\\\":2.70},\\n    ]\\n    \\ndf \\u003d pd.DataFrame(d)\\nprint(df)\\nprint(len(df.columns))\\nprint(len(df))\\nfsdf \\u003d spark.createDataFrame(d)\\n\\nfsdf.show(5)\\nfsdf.createOrReplaceTempView(\\\"table_1\\\")\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":1,\"componentName\":\"Code 1\",\"tableName\":\"table_1\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\n\\nd \\u003d [\\n    {\\\"c1\\\":\\\"ghg\\\", \\\"c2_\\\": 25, \\\"c3\\\":38.9},\\n    {\\\"c1\\\":\\\"ffg\\\", \\\"c2_\\\": 45, \\\"c3\\\":5.89},\\n    {\\\"c1\\\":\\\"erdd\\\", \\\"c2_\\\": 43, \\\"c3\\\":64.8},\\n    {\\\"c1\\\":\\\"rtre\\\", \\\"c2_\\\": 22, \\\"c3\\\":5.78},\\n    {\\\"c1\\\":\\\"ewss\\\", \\\"c2_\\\": 67, \\\"c3\\\":27.7},\\n        {\\\"c1\\\":\\\"ewss\\\", \\\"c2_\\\": 67, \\\"c3\\\":27.7},\\n\\n    ]\\nfsdf \\u003d spark.createDataFrame(d)\\n\\nfsdf.show(5)\\nfsdf.createOrReplaceTempView(\\\"table_2\\\")\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":2,\"componentName\":\"Code 2\",\"tableName\":\"table_2\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"excludeNotification\":\"N\"},{\"pluginId\":\"244a065c-88bd-4437-8ff9-f25970876ace\",\"pluginName\":\"Export data compare results to shared folder\",\"pluginOptions\":{\"parameters\":[{\"parameterId\":\"eae5284a-dfce-4c2b-9e1f-2e1b67087ace\",\"fieldName\":\"work_book\",\"displayName\":\"Enter excel file name\",\"value\":\"result123\",\"dataType\":\"String\"},{\"parameterId\":\"1c1f07e3-ebfc-4045-88ec-e8dc7f202ace\",\"fieldName\":\"component_name\",\"displayName\":\"Enter component name\",\"value\":\"Data Compare 3\",\"dataType\":\"String\"},{\"parameterId\":\"996ef056-70d1-424a-91b5-335884b3dace\",\"fieldName\":\"source\",\"displayName\":\"Enter alias name for source\",\"value\":\"source_tab\",\"dataType\":\"String\"},{\"parameterId\":\"dfbd6427-5838-4a70-8fa8-c969735b9ace\",\"fieldName\":\"target\",\"displayName\":\"Enter alias name for target\",\"value\":\"target_tab\",\"dataType\":\"String\"}],\"inputDatasets\":[],\"outputDatasets\":[],\"dataSources\":[{\"paramId\":\"1cbecd7e-e335-4689-8f2b-668ae2c9887c\",\"fieldName\":\"data_source\",\"displayName\":\"Select Shared Folder Data Source\",\"value\":\"2033\",\"dataSourceName\":\"shared_folder_upendar\"}]},\"componentId\":13,\"componentName\":\"Plugin 13\",\"tableName\":\"Plugin_13\",\"category\":\"Processor\",\"componentType\":\"Plugin\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.PluginComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"pluginId\":\"c5380c06-8f61-4e32-9f97-26050aa08815\",\"pluginName\":\"Write multiple datasets into a single excel file in S3\",\"pluginOptions\":{\"parameters\":[{\"parameterId\":\"d7554b1b-2a8b-4464-ad3f-fe4201158033\",\"fieldName\":\"work_book\",\"displayName\":\"Enter Excel File Name\",\"value\":\"upendar123\",\"dataType\":\"String\"}],\"inputDatasets\":[{\"inputDatasetId\":\"5c709038-41b9-479c-9593-46e10b03c5fa\",\"fieldName\":\"target_duplicate\",\"displayName\":\"Select Target Duplicate Dataset\"},{\"inputDatasetId\":\"a319ad51-8c24-4ed7-a5eb-6e0e27cd215b\",\"fieldName\":\"source_duplicate\",\"displayName\":\"Select Source Duplicate Dataset\"},{\"inputDatasetId\":\"2228036e-4bb9-4705-abe5-75554e585671\",\"fieldName\":\"only_target\",\"displayName\":\"Select Only Target Dataset\"},{\"inputDatasetId\":\"8cf9d90f-fba5-4625-9cae-817a8e063b66\",\"fieldName\":\"only_source\",\"displayName\":\"Select Only Source Dataset\"},{\"inputDatasetId\":\"8ffd195a-d98b-468a-a70d-d406a3990759\",\"fieldName\":\"differences\",\"displayName\":\"Select Difference Dataset\"},{\"inputDatasetId\":\"0eff0e6e-f5f0-42ec-92a2-81613a8ccfaf\",\"fieldName\":\"matched\",\"displayName\":\"Select Matched Dataset\"}],\"outputDatasets\":[],\"dataSources\":[{\"paramId\":\"03d64e38-161a-4767-bf2a-9df4ee29e8be\",\"fieldName\":\"source_datasource\",\"displayName\":\"Select Excel Data Source\",\"value\":\"1923\",\"dataSourceName\":\"EXCEL aws s3 upendar\"}]},\"componentId\":19,\"componentName\":\"Plugin 19\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Plugin\",\"rank\":0,\"displayRows\":0,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.PluginComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\nimport json\\r\\nimport math\\r\\nimport os\\r\\nimport time\\r\\nfrom datetime import datetime\\r\\nimport traceback\\r\\nimport pandas as pd\\r\\nimport smbclient\\r\\n\\r\\nt1\\u003dtime.time()\\r\\ntry:\\r\\n    \\r\\n    java_object \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getConnectionParams(\\\"shared_folder_upendar\\\",\\\"read\\\",81)\\r\\n    \\r\\n    data_source_details_list \\u003d [java_object.toList().apply(i) for i in range(java_object.size())]\\r\\n    data_source_details\\u003d {i._1(): i._2() for i in data_source_details_list}\\r\\n    \\r\\n    if data_source_details.get(\\\"impersonate\\\")\\u003d\\u003d\\\"true\\\":\\r\\n    \\tusername \\u003d data_source_details[\\\"userName\\\"]\\r\\n    \\tpassword \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getDecryptedPassword(data_source_details[\\\"password\\\"])\\r\\n    \\tsmbclient.ClientConfig(username\\u003dusername, password\\u003dpassword)\\r\\n    else:\\r\\n    \\tsmbclient.ClientConfig()\\r\\n    \\r\\n    component_name \\u003d \\\"Data Compare 3\\\"\\r\\n    file_location \\u003d data_source_details[\\\"path\\\"].split(\\\"smb:\\\")[-1]\\r\\n    file_name \\u003d \\\"result123\\\"+\\\".xlsx\\\"\\r\\n    source \\u003d \\\"source_tab\\\"\\r\\n    target \\u003d \\\"target_tab\\\"\\r\\n    \\r\\n    \\r\\n    date_folder \\u003d datetime.now().strftime(\\\"%Y/%b/%d\\\")\\r\\n    final_folder \\u003d os.path.join(file_location, \\\"Dataflow5240_upendar_v2\\\", date_folder, \\\"35066\\\")\\r\\n    \\r\\n    smbclient.makedirs(final_folder, exist_ok\\u003dTrue)\\r\\n    file_full_path \\u003d os.path.join(final_folder, file_name)\\r\\n    def highlight_cells(val, min_v, max_v):\\r\\n    \\tcolor \\u003d \\\"black\\\"\\r\\n    \\tif \\\"V : \\\" in str(val):\\r\\n    \\t\\tthreshold \\u003d float(val.split(\\\"V : \\\")[-1])\\r\\n    \\t\\tif max_v \\u003e threshold \\u003e min_v:\\r\\n    \\t\\t\\tcolor \\u003d \\u0027green\\u0027\\r\\n    \\t\\telse:\\r\\n    \\t\\t\\tcolor \\u003d \\\"red\\\"\\r\\n    \\r\\n    \\treturn \\u0027color: {}\\u0027.format(color)\\r\\n    \\r\\n    \\r\\n    scala_output \\u003d json.loads(spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getDataCompareDownloadDetails(35066, component_name))\\r\\n    \\r\\n    scala_output[\\\"column_validations\\\"] \\u003d scala_output[\\\"column_validations\\\"] if scala_output.get(\\\"column_validations\\\") else {}\\r\\n    component_dataset_name \\u003d scala_output[\\\"dataset_name\\\"]\\r\\n    \\r\\n    created_date \\u003d datetime.strptime(scala_output[\\\"run_date\\\"], \\\"%Y-%m-%d %H:%M:%S.%f\\\").strftime(\\\"%m/%d/%Y %H:%M:%S %p\\\")\\r\\n    dataset_name_to_sheet_name \\u003d {\\r\\n    \\tf\\\"{component_dataset_name}_Matched\\\": \\\"Matched\\\",\\r\\n    \\tf\\\"{component_dataset_name}_Source_Duplicate\\\": f\\\"Duplicates in Source-{source}\\\"[:31],\\r\\n    \\tf\\\"{component_dataset_name}_Target_Duplicate\\\": f\\\"Duplicates in Target-{target}\\\"[:31],\\r\\n    \\tf\\\"{component_dataset_name}_Difference\\\": \\\"Differences\\\",\\r\\n    \\tf\\\"{component_dataset_name}_OnlySource\\\": f\\\"Only in Source-{source}\\\"[:31],\\r\\n    \\tf\\\"{component_dataset_name}_OnlyTarget\\\": f\\\"Only in Target-{target}\\\"[:31]\\r\\n    }\\r\\n    \\r\\n    third_line_of_sheet \\u003d {\\r\\n    \\tf\\\"{component_dataset_name}_Matched\\\": f\\\"Matched Between (S) {source} and (T) {target}\\\",\\r\\n    \\tf\\\"{component_dataset_name}_Source_Duplicate\\\": f\\\"Duplicates in Source-{source}\\\",\\r\\n    \\tf\\\"{component_dataset_name}_Target_Duplicate\\\": f\\\"Duplicates in Target-{target}\\\",\\r\\n    \\tf\\\"{component_dataset_name}_Difference\\\": f\\\"Differences Between (S) {source} and (T) {target}\\\",\\r\\n    \\tf\\\"{component_dataset_name}_OnlySource\\\": f\\\"Data Only in Source {source}\\\",\\r\\n    \\tf\\\"{component_dataset_name}_OnlyTarget\\\": f\\\"Data Only in Target {target}\\\"\\r\\n    }\\r\\n    \\r\\n    status_mapping \\u003d {\\\"Ok\\\": \\\"Success\\\", \\\"Failed\\\": \\\"Fail\\\"}\\r\\n    \\r\\n    name_mapping \\u003d {\\r\\n    \\t\\\"only_source\\\": f\\\"Data Only in Source: {source}\\\",\\r\\n    \\t\\\"source_duplicates\\\": f\\\"Duplicates in Source: {source}\\\",\\r\\n    \\t\\\"only_target\\\": f\\\"Data Only in Target: {target}\\\",\\r\\n    \\t\\\"target_duplicates\\\": f\\\"Duplicates in Target: {target}\\\",\\r\\n    }\\r\\n    \\r\\n    no_change_dataset_names \\u003d [f\\\"{component_dataset_name}_OnlySource\\\",\\r\\n    \\t\\t\\t\\t\\t\\t   f\\\"{component_dataset_name}_OnlyTarget\\\", f\\\"{component_dataset_name}_Source_Duplicate\\\",\\r\\n    \\t\\t\\t\\t\\t\\t   f\\\"{component_dataset_name}_Target_Duplicate\\\"]\\r\\n    matched_dataset \\u003d f\\\"{component_dataset_name}_Matched\\\"\\r\\n    difference_dataset \\u003d f\\\"{component_dataset_name}_Difference\\\"\\r\\n    \\r\\n    \\r\\n    # with open(file_full_path, \\\"wb\\\") as f:\\r\\n    with smbclient.open_file(file_full_path, \\\"wb\\\") as f:\\r\\n    \\twriter \\u003d pd.ExcelWriter(f, engine\\u003d\\\"xlsxwriter\\\")\\r\\n    \\tworkbook \\u003d writer.book\\r\\n    \\r\\n    \\tblue_background_bold_white \\u003d workbook.add_format({\\u0027bg_color\\u0027: \\u0027#0066CC\\u0027, \\u0027bold\\u0027: True, \\u0027font_color\\u0027: \\\"white\\\"})\\r\\n    \\tyellow_background_bold \\u003d workbook.add_format({\\u0027bg_color\\u0027: \\u0027yellow\\u0027, \\u0027bold\\u0027: True})\\r\\n    \\tred_font_color \\u003d workbook.add_format({\\u0027font_color\\u0027: \\\"red\\\"})\\r\\n    \\tred_font_color_bold \\u003d workbook.add_format({\\u0027font_color\\u0027: \\\"red\\\",\\u0027bold\\u0027: True})\\r\\n    \\tgreen_font_color \\u003d workbook.add_format({\\u0027font_color\\u0027: \\\"green\\\"})\\r\\n    \\tbold \\u003d workbook.add_format({\\u0027bold\\u0027: True})\\r\\n    \\talign_center_bold \\u003d workbook.add_format({\\u0027align\\u0027: \\u0027center\\u0027, \\u0027bold\\u0027: True})\\r\\n    \\talign_center \\u003d workbook.add_format({\\u0027align\\u0027: \\u0027center\\u0027})\\r\\n    \\ttext_wrap \\u003d workbook.add_format({\\u0027align\\u0027: \\u0027left\\u0027, \\u0027valign\\u0027:\\u0027top\\u0027})\\r\\n    \\ttext_wrap.set_text_wrap()\\r\\n    \\tpercent_format \\u003d workbook.add_format()\\r\\n    \\tpercent_format.set_num_format(\\\"0.00%\\\")\\r\\n    \\r\\n    \\t# summary sheet\\r\\n    \\tworksheet \\u003d workbook.add_worksheet(name\\u003d\\\"Summary\\\")\\r\\n    \\tsecond_line \\u003d f\\\"{source} (S) / {target} (T)\\\"\\r\\n    \\tworksheet.merge_range(0, 0, 0, 3, scala_output[\\u0027dataflow_name\\u0027], align_center_bold)\\r\\n    \\tworksheet.merge_range(1, 0, 1, 3, second_line, align_center_bold)\\r\\n    \\tworksheet.merge_range(2, 0, 2, 3, f\\\"Summary Totals - {created_date}\\\", align_center)\\r\\n    \\r\\n    \\tworksheet.write(5, 0, f\\\"Source: {source} Count : {scala_output[\\u0027source_dataset\\u0027][\\u0027count\\u0027]}\\\", bold)\\r\\n    \\tworksheet.write(6, 0, f\\\"Target: {target} Count : {scala_output[\\u0027target_dataset\\u0027][\\u0027count\\u0027]}\\\", bold)\\r\\n    \\tif scala_output[\\u0027difference_count\\u0027]:\\r\\n    \\t\\tworksheet.write(7, 0,\\r\\n    \\t\\t\\t\\t\\t\\tf\\\"Data difference in both Source: {source} and Target: {target} : {scala_output[\\u0027difference_count\\u0027]}\\\",\\r\\n    \\t\\t\\t\\t\\t\\tred_font_color_bold)\\r\\n    \\telse:\\r\\n    \\t\\tworksheet.write(7, 0,\\r\\n    \\t\\t\\t\\t\\t\\tf\\\"Data difference in both Source: {source} and Target: {target} : {scala_output[\\u0027difference_count\\u0027]}\\\",\\r\\n    \\t\\t\\t\\t\\t\\tbold)\\r\\n    \\tworksheet.write(8, 0, f\\\"Matched Data : {scala_output[\\u0027matched_count\\u0027]}\\\", bold)\\r\\n    \\r\\n    \\tfor c, i in enumerate([\\\"Name\\\", \\\"Total\\\", \\\"percentage(%)\\\", \\\"Status\\\"]):\\r\\n    \\t\\tworksheet.write(9, c, i, blue_background_bold_white)\\r\\n    \\r\\n    \\tfor c, i in enumerate([\\\"only_source\\\", \\\"source_duplicates\\\", \\\"only_target\\\", \\\"target_duplicates\\\"]):\\r\\n    \\t\\tworksheet.write(10 + c, 0, name_mapping[i])\\r\\n    \\t\\tworksheet.write(10 + c, 1, scala_output[i][\\\"count\\\"])\\r\\n    \\t\\tworksheet.write(10 + c, 2, scala_output[i][\\\"percent\\\"] / 100, percent_format)\\r\\n    \\t\\tif scala_output[i][\\\"status\\\"] \\u003d\\u003d \\\"Failed\\\":\\r\\n    \\t\\t\\tcell_font_color \\u003d red_font_color\\r\\n    \\t\\telse:\\r\\n    \\t\\t\\tcell_font_color \\u003d green_font_color\\r\\n    \\r\\n    \\t\\tworksheet.write(10 + c, 3, status_mapping[scala_output[i][\\\"status\\\"]], cell_font_color)\\r\\n    \\r\\n    \\tworksheet.set_column(0, 0, len(f\\\"Data difference in both Source: {source} and Target: {target}\\\"))\\r\\n    \\tworksheet.set_column(1, 1, len(\\\"Total\\\"))\\r\\n    \\tworksheet.set_column(2, 2, len(\\\"percentage(%)\\\"))\\r\\n    \\tworksheet.set_column(3, 3, len(\\\"Status\\\"))\\r\\n    \\r\\n    \\t# query sheet\\r\\n    \\tworksheet \\u003d workbook.add_worksheet(name\\u003d\\\"Queries\\\")\\r\\n    \\tworksheet.write(0, 0, scala_output[\\u0027dataflow_name\\u0027], align_center_bold)\\r\\n    \\tworksheet.write(1, 0, second_line, align_center_bold)\\r\\n    \\tqueries_third_line \\u003d f\\\"Queries for {source} (S) / {target} (T)\\\"\\r\\n    \\tworksheet.write(2, 0, queries_third_line, align_center)\\r\\n    \\r\\n    \\tworksheet.write(4, 0, f\\\"{source} Query (S)\\\", blue_background_bold_white)\\r\\n    \\tworksheet.write(5, 0, scala_output[\\u0027source_dataset\\u0027][\\u0027query\\u0027], text_wrap)\\r\\n    \\tworksheet.write(7, 0, f\\\"{target} Query (T)\\\", blue_background_bold_white)\\r\\n    \\tworksheet.write(8, 0, scala_output[\\u0027target_dataset\\u0027][\\u0027query\\u0027], text_wrap)\\r\\n    \\r\\n    \\tworksheet.set_column(0, 0, max([len(_) for _ in [scala_output[\\u0027dataflow_name\\u0027], queries_third_line]]))\\r\\n    \\tworksheet.set_row(5, 200)\\r\\n    \\tworksheet.set_row(8, 200)\\r\\n    \\tstart_header \\u003d 4\\r\\n    \\tstart_data \\u003d 5\\r\\n    \\r\\n    \\t#  _Matched, _OnlySource, _OnlyTarget, _Source_Duplicate, _Target_Duplicate, _Difference\\r\\n    \\tfor dataset_name in [matched_dataset]+no_change_dataset_names+[difference_dataset]:\\r\\n    \\t\\tsheet_name \\u003d dataset_name_to_sheet_name[dataset_name]\\r\\n    \\t\\ttry:\\r\\n    \\t\\t\\tsdf \\u003d spark.sql(\\\"select * from {}\\\".format(dataset_name))\\r\\n    \\t\\texcept:\\r\\n    \\t\\t\\tworksheet \\u003d workbook.add_worksheet(name\\u003dsheet_name)\\r\\n    \\t\\t\\tworksheet.write(0, 0, scala_output[\\u0027dataflow_name\\u0027], align_center_bold)\\r\\n    \\t\\t\\tworksheet.write(1, 0, second_line, align_center_bold)\\r\\n    \\t\\t\\tworksheet.write(2, 0, third_line_of_sheet[dataset_name], align_center)\\r\\n    \\t\\t\\terror_msg \\u003d f\\\"Note: No data available for dataset {sheet_name}\\\"\\r\\n    \\t\\t\\tmerged_column_width \\u003d max([len(_) for _ in [scala_output[\\u0027dataflow_name\\u0027], second_line, error_msg, third_line_of_sheet[dataset_name]]])\\r\\n    \\t\\t\\tworksheet.set_row(start_header, cell_format\\u003dyellow_background_bold)\\r\\n    \\t\\t\\tworksheet.set_column(0, 0, merged_column_width)\\r\\n    \\t\\t\\tcontinue\\r\\n    \\r\\n    \\t\\tdf \\u003d sdf.toPandas()\\r\\n    \\t\\tno_of_rows \\u003d len(df)\\r\\n    \\t\\tno_of_columns \\u003d len(df.columns)\\r\\n    \\t\\tfirst_time \\u003d True\\r\\n    \\t\\tif dataset_name.endswith(\\\"_Difference\\\"):\\r\\n    \\t\\t\\tfor column_name, rules in scala_output[\\u0027column_validations\\u0027].items():\\r\\n    \\t\\t\\t\\tmin_v \\u003d rules[\\\"lower\\\"]\\r\\n    \\t\\t\\t\\tmax_v \\u003d rules[\\\"upper\\\"]\\r\\n    \\t\\t\\t\\tif first_time:\\r\\n    \\t\\t\\t\\t\\tdf_style \\u003d df.style.applymap(highlight_cells, subset\\u003dpd.IndexSlice[:, [column_name]], min_v\\u003dmin_v,\\r\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t max_v\\u003dmax_v)\\r\\n    \\t\\t\\t\\t\\tfirst_time \\u003d False\\r\\n    \\t\\t\\t\\telse:\\r\\n    \\t\\t\\t\\t\\tdf_style \\u003d df_style.applymap(highlight_cells, subset\\u003dpd.IndexSlice[:, [column_name]], min_v\\u003dmin_v,\\r\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t max_v\\u003dmax_v)\\r\\n    \\r\\n    \\t\\tif first_time:\\r\\n    \\t\\t\\tdf.to_excel(writer, sheet_name\\u003dsheet_name, index\\u003dFalse, startrow\\u003dstart_data, header\\u003dFalse)\\r\\n    \\t\\telse:\\r\\n    \\t\\t\\tdf_style.to_excel(writer, sheet_name\\u003dsheet_name, index\\u003dFalse, startrow\\u003dstart_data, header\\u003dFalse)\\r\\n    \\t\\tworksheet \\u003d writer.sheets[sheet_name]\\r\\n    \\t\\tworksheet.merge_range(0, 0, 0, no_of_columns-1, scala_output[\\u0027dataflow_name\\u0027], align_center_bold)\\r\\n    \\t\\tworksheet.merge_range(1, 0, 1, no_of_columns-1, second_line, align_center_bold)\\r\\n    \\t\\tworksheet.merge_range(2, 0, 2, no_of_columns-1, third_line_of_sheet[dataset_name], align_center)\\r\\n    \\t\\tmerged_column_width \\u003d max([len(_) for _ in [scala_output[\\u0027dataflow_name\\u0027], second_line, third_line_of_sheet[dataset_name]]])\\r\\n    \\t\\tcolumn_widths \\u003d {}\\r\\n    \\t\\tfor col_number, column_name in enumerate(df.columns.values):\\r\\n    \\t\\t\\tworksheet.write(start_header, col_number, column_name)\\r\\n    \\t\\t\\tif df.empty:\\r\\n    \\t\\t\\t\\tcolumn_width \\u003d len(column_name)\\r\\n    \\t\\t\\telse:\\r\\n    \\t\\t\\t\\tcolumn_width \\u003d max(df[column_name].astype(str).map(len).max(), len(column_name))\\r\\n    \\t\\t\\tcolumn_widths[col_number] \\u003d column_width\\r\\n    \\t\\t\\tif dataset_name.endswith(\\\"_Matched\\\") and column_name in scala_output[\\\"column_validations\\\"].keys():\\r\\n    \\t\\t\\t\\tworksheet.conditional_format(start_data, col_number, no_of_rows + start_header, col_number,\\r\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t {\\u0027type\\u0027: \\u0027text\\u0027,\\r\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \\u0027criteria\\u0027: \\u0027begins with\\u0027,\\r\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \\u0027value\\u0027: \\u0027A : \\u0027,\\r\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \\u0027format\\u0027: green_font_color})\\r\\n    \\t\\t\\telif dataset_name.endswith(\\\"_Difference\\\") and column_name not in scala_output[\\\"column_validations\\\"].keys():\\r\\n    \\t\\t\\t\\tworksheet.conditional_format(start_data, col_number, no_of_rows + start_header, col_number,\\r\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t {\\u0027type\\u0027: \\u0027text\\u0027,\\r\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \\u0027criteria\\u0027: \\u0027begins with\\u0027,\\r\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \\u0027value\\u0027: \\u0027A : \\u0027,\\r\\n    \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \\u0027format\\u0027: red_font_color})\\r\\n    \\t\\tif sum(column_widths.values()) \\u003c merged_column_width:\\r\\n    \\t\\t\\tdiff \\u003d merged_column_width - sum(column_widths.values())\\r\\n    \\t\\t\\tpatch \\u003d math.ceil(diff / no_of_columns)\\r\\n    \\t\\t\\tcolumn_widths \\u003d {k: v + patch for k, v in column_widths.items()}\\r\\n    \\t\\tfor col_number, column_width in column_widths.items():\\r\\n    \\t\\t\\tworksheet.set_column(col_number, col_number, column_width)\\r\\n    \\r\\n    \\t\\tworksheet.set_row(start_header, cell_format\\u003dblue_background_bold_white)\\r\\n    \\r\\n    \\r\\n    \\twriter.close()\\r\\n    \\r\\n    print(f\\\"Excel File({file_full_path}) Created Successfully\\\")\\r\\n    \\r\\n    print(time.time()-t1)\\r\\n    \\r\\nexcept:\\r\\n    print(traceback.format_exc())\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":18,\"componentName\":\"Code 18\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\nimport os\\r\\nimport boto3 as boto3\\r\\nimport pandas as pd\\r\\nfrom io import BytesIO\\r\\nfrom datetime import datetime\\r\\nimport traceback\\r\\ntry:\\r\\n    \\r\\n    java_object \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getConnectionParams(\\\"EXCEL aws s3 upendar\\\",\\\"read\\\")\\r\\n    \\r\\n    data_source_details_list \\u003d [java_object.toList().apply(i) for i in range(java_object.size())]\\r\\n    data_source_details\\u003d {i._1(): i._2() for i in data_source_details_list}\\r\\n    \\r\\n    access_key \\u003d data_source_details[\\\"aws-access-key\\\"]\\r\\n    secret_key \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getDecryptedPassword(data_source_details[\\\"aws-secret-key\\\"])\\r\\n    region_name \\u003d data_source_details.get(\\\"region\\\")\\r\\n    bucket_name \\u003d data_source_details[\\\"aws-bucket-name\\\"]\\r\\n    date_folder \\u003d datetime.now().strftime(\\\"%Y/%b/%d\\\")\\r\\n    file_path \\u003d os.path.join(data_source_details[\\\"path\\\"],\\\"Dataflow5240_upendar_v2\\\", date_folder,\\\"35066\\\", \\\"upendar123\\\"+\\\".xlsx\\\")\\r\\n    \\r\\n    \\r\\n    s3 \\u003d boto3.resource(\\r\\n    \\tservice_name\\u003d\\\"s3\\\",\\r\\n    \\tregion_name\\u003dregion_name,\\r\\n    \\taws_access_key_id\\u003daccess_key,\\r\\n    \\taws_secret_access_key\\u003dsecret_key\\r\\n    )\\r\\n    \\r\\n    excel_file \\u003d BytesIO()\\r\\n    \\r\\n     \\r\\n    dfs\\u003d[\\\"Data_Compare_3_Matched\\\",\\\"Data_Compare_3_Difference\\\",\\\"Data_Compare_3_OnlySource\\\",\\\"Data_Compare_3_OnlyTarget\\\",\\\"Data_Compare_3_Source_Duplicate\\\",\\\"Data_Compare_3_Target_Duplicate\\\"]\\r\\n    \\r\\n    with pd.ExcelWriter(excel_file, engine\\u003d\\\"openpyxl\\\") as writer:\\r\\n    \\tfor df in dfs:\\r\\n    \\t\\tdf1\\u003dspark.sql(\\\"select * from {}\\\".format(df))\\r\\n    \\t\\tdf1.toPandas()[:1000000].to_excel(writer,sheet_name\\u003ddf,index\\u003dFalse)\\r\\n     \\r\\n    excel_file.seek(0)\\r\\n    s3.Bucket(bucket_name).put_object(Key\\u003dfile_path, Body\\u003dexcel_file.read())\\r\\n    \\r\\n    print(f\\\"Excel File({os.path.join(bucket_name, file_path)}) Created Successfully.\\\")\\r\\nexcept:\\r\\n    print(traceback.format_exc())\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":20,\"componentName\":\"Code 20\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"sourceDataFrame\":\"table_1\",\"targetDataFrame\":\"table_2\",\"mapping\":[{\"sourceColumn\":\"c1\",\"sourceType\":\"string\",\"targetColumn\":\"c1\",\"targetType\":\"string\",\"unique\":\"Y\",\"sourceDataFormat\":\"\",\"targetDataFormat\":\"\",\"sourceTypeChange\":\"N\",\"targetTypeChange\":\"N\"},{\"sourceColumn\":\"c2\",\"sourceType\":\"long\",\"targetColumn\":\"c2_\",\"targetType\":\"long\",\"unique\":\"N\",\"sourceDataFormat\":\"\",\"targetDataFormat\":\"\",\"sourceTypeChange\":\"N\",\"targetTypeChange\":\"N\",\"upperThreshold\":10.0,\"lowerThreshold\":-10.0,\"hasThreshold\":\"Y\",\"thresholdUnit\":\"percent\"},{\"sourceColumn\":\"c3\",\"sourceType\":\"double\",\"targetColumn\":\"c3\",\"targetType\":\"double\",\"unique\":\"N\",\"sourceDataFormat\":\"\",\"targetDataFormat\":\"\",\"sourceTypeChange\":\"N\",\"targetTypeChange\":\"N\",\"upperThreshold\":5.0,\"lowerThreshold\":-5.0,\"hasThreshold\":\"Y\",\"thresholdUnit\":\"number\"}],\"onlyInA\":\"Y\",\"onlyInB\":\"Y\",\"difference\":\"Y\",\"enableTrim\":\"N\",\"replaceNull\":\"N\",\"autoDataTypeConversion\":\"N\",\"columnDifferencesCount\":\"N\",\"ignoreCase\":\"N\",\"subDataFrames\":[{\"subDataFrameName\":\"_Source_Duplicate\",\"type\":\"Duplicates In A\"},{\"subDataFrameName\":\"_Target_Duplicate\",\"type\":\"Duplicates In B\"},{\"subDataFrameName\":\"_Matched\",\"type\":\"Matched\"},{\"subDataFrameName\":\"_OnlySource\",\"type\":\"Only In A\"},{\"subDataFrameName\":\"_OnlyTarget\",\"type\":\"Only In B\"},{\"subDataFrameName\":\"_Difference\",\"type\":\"Difference\"}],\"componentId\":3,\"componentName\":\"Data Compare 3\",\"tableName\":\"Data_Compare_3\",\"category\":\"Data Quality\",\"componentType\":\"Data Compare\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[2,1],\"options\":{},\"className\":\"com.datagaps.dataflow.models.DataCompareComponent\",\"isCheckpointEnabled\":\"N\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":20,"livyOptions":"{\"kind\":\"spark\",\"proxyUser\":\"\",\"jars\":[],\"pyFiles\":[],\"files\":[],\"driverMemory\":\"\",\"driverCores\":0,\"executorMemory\":\"\",\"executorCores\":0,\"numExecutors\":0,\"archives\":[],\"queue\":\"\",\"name\":\"\",\"conf\":{},\"heartbeatTimeoutInSecond\":0}","isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow/upendar","workSchemaName":null},"analysis":[],"datamodels":[],"tagDetails":[{"tagName":"this is for testing the runtime tag","additinalInfo":"{\"colourCode\":\"#008000\"}"}],"dataCompares":[]}