{"dataflow":{"dfKey":"28a3eb83-177e-438a-99c7-c1c893f6353a","name":"s3_excel_read_plugin","tags":null,"description":"","definition":"{\"name\":\"s3_excel_read_plugin\",\"description\":\"\",\"version\":0,\"livyServerId\":0,\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\nimport re\\r\\n\\r\\nimport boto3 as boto3\\r\\nimport pandas as pd\\r\\n\\r\\njava_object \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getConnectionParams(\\\"EXCEL aws s3 upendar\\\",\\\"read\\\")\\r\\n\\r\\ndata_source_details_list \\u003d [java_object.toList().apply(i) for i in range(java_object.size())]\\r\\ndata_source_details\\u003d {i._1(): i._2() for i in data_source_details_list}\\r\\n\\r\\naccess_key \\u003d data_source_details[\\\"aws-access-key\\\"]\\r\\nsecret_key \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getDecryptedPassword(data_source_details[\\\"aws-secret-key\\\"])\\r\\nregion_name \\u003d \\u0027us-east-2\\u0027\\r\\nbucket_name \\u003d data_source_details[\\\"aws-bucket-name\\\"]\\r\\nfile_path \\u003d data_source_details[\\\"path\\\"]+\\\"sample1.xls\\\"\\r\\n\\r\\n\\r\\n# access_key \\u003d \\u0027AKIAV5DLRL43XJPXFSP2\\u0027\\r\\n# secret_key \\u003d \\u0027dC5ODwLVldz8xB4VFTUVmx89gQrx1AmztKtw7ghR\\u0027\\r\\n# region_name \\u003d \\u0027us-east-2\\u0027\\r\\n# bucket_name \\u003d \\u0027datagapsqa\\u0027\\r\\n# file_path \\u003d \\\"data/sample1.xls\\\"\\r\\n\\r\\ns3 \\u003d boto3.resource(\\r\\n    service_name\\u003d\\u0027s3\\u0027,\\r\\n    region_name\\u003dregion_name,\\r\\n    aws_access_key_id\\u003daccess_key,\\r\\n    aws_secret_access_key\\u003dsecret_key\\r\\n)\\r\\nobj \\u003d s3.Bucket(bucket_name).Object(file_path).get()\\r\\ndownloaded_file \\u003d obj[\\u0027Body\\u0027].read()\\r\\ndf_dict \\u003d pd.read_excel(downloaded_file, sheet_name\\u003dNone, keep_default_na\\u003dFalse)\\r\\nprint()\\r\\nfor sheet_name, dataframe in df_dict.items():\\r\\n    spark_dataframe \\u003d spark.createDataFrame(dataframe.astype(\\u0027str\\u0027))\\r\\n    sheet_name \\u003d re.sub(\\\"[^0-9A-Za-z_]\\\", \\\"_\\\", sheet_name)\\r\\n    spark_dataframe.createOrReplaceTempView(sheet_name+\\\"_1\\\")\\r\\n    print(sheet_name)\\r\\n    spark_dataframe.show()\\r\\n    print()\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":1,\"componentName\":\"Code 1\",\"tableName\":\"code_1\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"pluginId\":\"ef5ef7cf-9d53-4d3f-bdee-5d178e36cdb4\",\"pluginName\":\"S3 excel read multiple sheets upendar\",\"pluginOptions\":{\"parameters\":[{\"parameterId\":\"95bc4c7c-5d3b-48cd-828b-217281a02b26\",\"fieldName\":\"access_key\",\"displayName\":\"Access Key\",\"value\":\"AKIAV5DLRL43XJPXFSP2\",\"dataType\":\"String\"},{\"parameterId\":\"a3190752-b061-47b7-8f67-fb8cc363f4ee\",\"fieldName\":\"secret_key\",\"displayName\":\"Secret Key\",\"value\":\"dC5ODwLVldz8xB4VFTUVmx89gQrx1AmztKtw7ghR\",\"dataType\":\"String\"},{\"parameterId\":\"4ecb0ea4-4b94-47d5-aed3-18754ca4fee0\",\"fieldName\":\"region_name\",\"displayName\":\"Region Name\",\"value\":\"us-east-2\",\"dataType\":\"String\"},{\"parameterId\":\"6c69dec6-1bfb-4bda-95b8-8dc0d15dd766\",\"fieldName\":\"bucket_name\",\"displayName\":\"Bucket Name\",\"value\":\"datagapsqa\",\"dataType\":\"String\"},{\"parameterId\":\"191814bd-7247-4268-b230-c18ee6577ba3\",\"fieldName\":\"file_name\",\"displayName\":\"File Name\",\"value\":\"sample1.xls\",\"dataType\":\"String\"},{\"parameterId\":\"819d3e5f-b3ae-497f-b2b1-f8311145be61\",\"fieldName\":\"component_name\",\"displayName\":\"Component Name\",\"value\":\"Plugin 2\",\"dataType\":\"String\"}],\"inputDatasets\":[],\"outputDatasets\":[],\"dataSources\":[{\"paramId\":\"b0be12df-74ef-4359-8335-38832cec927c\",\"fieldName\":\"data_source\",\"displayName\":\"Select Excel Data Source\",\"value\":\"1923\"}]},\"componentId\":2,\"componentName\":\"Plugin 2\",\"tableName\":\"Plugin_2\",\"category\":\"Processor\",\"componentType\":\"Plugin\",\"rank\":0,\"displayRows\":0,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.PluginComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"pluginId\":\"d2d28fbb-721c-4a2c-8b10-321c86b5aabc\",\"pluginName\":\"S3 excel read single sheet upendar abc\",\"pluginOptions\":{\"parameters\":[{\"parameterId\":\"191814bd-7247-4268-b230-c18ee6577abc\",\"fieldName\":\"file_name\",\"displayName\":\"File Name\",\"value\":\"sample1.xls\",\"dataType\":\"String\"},{\"parameterId\":\"5a01fc27-008d-4332-a0ab-4d54f986fabc\",\"fieldName\":\"sheet_name\",\"displayName\":\"Sheet Name\",\"value\":\"Example Test\",\"dataType\":\"String\"},{\"parameterId\":\"3b272272-6cf9-419d-837b-3c3c23ca7c16\",\"fieldName\":\"test123\",\"displayName\":\"test123\",\"value\":\"None\",\"dataType\":\"text\"},{\"parameterId\":\"3da443ac-2987-4115-9a9d-0f54e469ca9a\",\"fieldName\":\"test234\",\"displayName\":\"test234\",\"value\":\"hjhj\",\"dataType\":\"String\"}],\"inputDatasets\":[],\"outputDatasets\":[{\"outputDatasetId\":\"a744d4ed-07a6-4a24-ac3c-ab52ec1c7abc\",\"fieldName\":\"output_dataset_name\",\"displayName\":\"Output dataset name\",\"value\":\"single_sheet1\"}],\"dataSources\":[{\"paramId\":\"3f646bef-2022-46b4-b4b1-9161a4cfdabc\",\"fieldName\":\"data_source\",\"displayName\":\"Select Excel Data Source\",\"value\":\"1923\"}]},\"componentId\":3,\"componentName\":\"Plugin 3\",\"tableName\":\"Plugin_3\",\"category\":\"Processor\",\"componentType\":\"Plugin\",\"rank\":0,\"displayRows\":0,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.PluginComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\nimport re\\r\\nimport os\\r\\nimport json\\r\\nimport boto3 as boto3\\r\\nimport pandas as pd\\r\\nimport traceback\\r\\ntry:    \\r\\n    sheet_names \\u003d [0,1]\\r\\n    if sheet_names is None or type(sheet_names) \\u003d\\u003d list:\\r\\n        if type(sheet_names) \\u003d\\u003d list:\\r\\n            if len(sheet_names)\\u003d\\u003d0:\\r\\n                raise Exception(\\\"sheet names should not be empty list\\\")\\r\\n    else:\\r\\n        raise Exception(\\\"sheet names should be list or None\\\")\\r\\n    spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.deleteDatasetInfo(json.dumps([]),20501,\\\"Plugin 2\\\")\\r\\n    \\r\\n    java_object \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getConnectionParams(\\\"EXCEL aws s3 upendar\\\",\\\"read\\\")\\r\\n    \\r\\n    data_source_details_list \\u003d [java_object.toList().apply(i) for i in range(java_object.size())]\\r\\n    data_source_details\\u003d {i._1(): i._2() for i in data_source_details_list}\\r\\n    \\r\\n    access_key \\u003d data_source_details[\\\"aws-access-key\\\"]\\r\\n    secret_key \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getDecryptedPassword(data_source_details[\\\"aws-secret-key\\\"])\\r\\n    region_name \\u003d data_source_details.get(\\\"region\\\")\\r\\n    bucket_name \\u003d data_source_details[\\\"aws-bucket-name\\\"]\\r\\n    file_path \\u003d os.path.join(data_source_details[\\\"path\\\"], \\\"sample1.xls\\\")\\r\\n    header \\u003d 0 if data_source_details.get(\\\"useHeader\\\")\\u003d\\u003d\\\"true\\\" else None\\r\\n    \\r\\n    s3 \\u003d boto3.resource(\\r\\n        service_name\\u003d\\u0027s3\\u0027,\\r\\n        region_name\\u003dregion_name,\\r\\n        aws_access_key_id\\u003daccess_key,\\r\\n        aws_secret_access_key\\u003dsecret_key\\r\\n    )\\r\\n    obj \\u003d s3.Bucket(bucket_name).Object(file_path).get()\\r\\n    downloaded_file \\u003d obj[\\u0027Body\\u0027].read()\\r\\n    df_dict \\u003d pd.read_excel(downloaded_file, sheet_name\\u003dsheet_names, header\\u003dheader)\\r\\n    \\r\\n    print()\\r\\n    for sheet_name, dataframe in df_dict.items():\\r\\n        print(sheet_name)\\r\\n        spark_dataframe \\u003d spark.createDataFrame(dataframe.astype(\\u0027str\\u0027))\\r\\n        sheet_name \\u003d re.sub(\\\"[^0-9A-Za-z_]\\\", \\\"_\\\", str(sheet_name))\\r\\n        spark_dataframe.createOrReplaceTempView(sheet_name)\\r\\n        print(sheet_name)\\r\\n        spark_dataframe.show()\\r\\n        print()\\r\\n        spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(spark_dataframe._jdf,sheet_name,\\\"Plugin 2\\\",20501)\\r\\nexcept:\\r\\n    print(traceback.format_exc())\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":4,\"componentName\":\"Code 4\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"pluginId\":\"11e9b761-611c-416e-8fa7-b258cc2f421b\",\"pluginName\":\"S3 excel read single and multiple sheets\",\"pluginOptions\":{\"parameters\":[{\"parameterId\":\"191814bd-7247-4268-b230-c18ee6577ba3\",\"fieldName\":\"file_name\",\"displayName\":\"File Name\",\"value\":\"sample1.xls\",\"dataType\":\"String\"},{\"parameterId\":\"819d3e5f-b3ae-497f-b2b1-f8311145be61\",\"fieldName\":\"component_name\",\"displayName\":\"Component Name\",\"value\":\"Plugin 2\",\"dataType\":\"String\"},{\"parameterId\":\"23dca209-bcd9-429c-8949-eb6666e411c2\",\"fieldName\":\"sheet_names\",\"displayName\":\"Sheet names\",\"value\":\"None\",\"dataType\":\"text\"}],\"inputDatasets\":[],\"outputDatasets\":[],\"dataSources\":[{\"paramId\":\"b0be12df-74ef-4359-8335-38832cec927c\",\"fieldName\":\"data_source\",\"displayName\":\"Select Excel Data Source\",\"value\":\"1923\"}]},\"componentId\":5,\"componentName\":\"Plugin 5\",\"tableName\":\"Plugin_5\",\"category\":\"Processor\",\"componentType\":\"Plugin\",\"rank\":0,\"displayRows\":0,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.PluginComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"partitionOptions\":{\"partitionType\":\"\",\"name\":\"\"},\"easyQueryDefJson\":{\"sqlQuery\":\"\"},\"componentId\":6,\"componentName\":\"SQL 6\",\"tableName\":\"SQL_6\",\"category\":\"Processor\",\"componentType\":\"SQL\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"dbTable\":\"select * from Plugin_2_Readme\"},\"className\":\"com.datagaps.dataflow.models.SQLComponent\",\"isCheckpointEnabled\":\"N\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\nimport re\\r\\nimport os\\r\\nimport json\\r\\nimport boto3 as boto3\\r\\nimport databricks.koalas as ks\\r\\nfrom collections import defaultdict\\r\\nfrom pyspark.sql.types import *\\r\\n\\r\\nsheet_names \\u003d None\\r\\nif sheet_names is None or type(sheet_names) \\u003d\\u003d list:\\r\\n\\tif type(sheet_names) \\u003d\\u003d list:\\r\\n\\t\\tif len(sheet_names)\\u003d\\u003d0:\\r\\n\\t\\t\\traise Exception(\\\"sheet names should not be empty list\\\")\\r\\nelse:\\r\\n\\traise Exception(\\\"sheet names should be list or None\\\")\\r\\n\\r\\njava_object \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getConnectionParams(\\\"EXCEL aws s3 upendar\\\",\\\"read\\\")\\r\\n\\r\\ndata_source_details_list \\u003d [java_object.toList().apply(i) for i in range(java_object.size())]\\r\\ndata_source_details\\u003d {i._1(): i._2() for i in data_source_details_list}\\r\\n\\r\\naccess_key \\u003d data_source_details[\\\"aws-access-key\\\"]\\r\\nsecret_key \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getDecryptedPassword(data_source_details[\\\"aws-secret-key\\\"])\\r\\nregion_name \\u003d data_source_details.get(\\\"region\\\")\\r\\nbucket_name \\u003d data_source_details[\\\"aws-bucket-name\\\"]\\r\\nfile_path \\u003d os.path.join(data_source_details[\\\"path\\\"], \\\"sample1.xls\\\")\\r\\nheader \\u003d 0 if data_source_details.get(\\\"useHeader\\\")\\u003d\\u003d\\\"true\\\" else None\\r\\n\\r\\ns3 \\u003d boto3.resource(\\r\\n\\tservice_name\\u003d\\u0027s3\\u0027,\\r\\n\\tregion_name\\u003dregion_name,\\r\\n\\taws_access_key_id\\u003daccess_key,\\r\\n\\taws_secret_access_key\\u003dsecret_key\\r\\n)\\r\\nobj \\u003d s3.Bucket(bucket_name).Object(file_path).get()\\r\\ndownloaded_file \\u003d obj[\\u0027Body\\u0027].read()\\r\\ndf_dict \\u003d ks.read_excel(downloaded_file, sheet_name\\u003dsheet_names, header\\u003dheader)\\r\\n\\r\\nprint()\\r\\nls\\u003d[]\\r\\ncomponent_name \\u003d \\\"Plugin 2\\\"\\r\\nfor sheet_name, dataframe in df_dict.items():\\r\\n\\tif dataframe.empty:\\r\\n\\t\\temp_rdd \\u003d spark.sparkContext.emptyRDD()\\r\\n\\t\\tcolumns \\u003d StructType([])\\r\\n\\t\\tspark_dataframe \\u003d spark.createDataFrame(data\\u003demp_rdd,\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tschema\\u003dcolumns)\\r\\n\\telse:\\r\\n# \\t\\tspark_dataframe \\u003d spark.createDataFrame(dataframe, infer_schema\\u003dFalse)\\r\\n\\t\\tspark_dataframe \\u003d dataframe.to_spark()\\r\\n\\r\\n\\r\\n\\tdataset_name \\u003d re.sub(\\\"[^0-9A-Za-z_]\\\", \\\"_\\\", f\\\"{component_name}_{sheet_name}\\\")\\r\\n\\tcolumn_map1 \\u003d defaultdict(list)\\r\\n\\tcolumn_map2 \\u003d {}\\r\\n\\tfor column in spark_dataframe.columns:\\r\\n\\t\\tm_column \\u003d re.sub(\\u0027[^0-9A-Za-z_]\\u0027, \\u0027_\\u0027, column)\\r\\n\\t\\tcolumn_map1[m_column].append(column)\\r\\n\\t\\tcolumn_map2[column] \\u003d m_column\\r\\n\\tfor n_column, o_columns in column_map1.items():\\r\\n\\t\\tif len(o_columns) \\u003e 1:\\r\\n\\t\\t\\tfor num, o_col in enumerate(o_columns):\\r\\n\\t\\t\\t\\tcolumn_map2[o_col] \\u003d f\\\"{n_column}_{num + 1}\\\"\\r\\n\\tspark_dataframe \\u003d spark_dataframe.toDF(*[column_map2[c] for c in spark_dataframe.columns])\\r\\n\\r\\n\\tspark_dataframe.createOrReplaceTempView(dataset_name)\\r\\n\\tprint(dataset_name)\\r\\n\\tls.append(dataset_name)\\r\\n\\tspark_dataframe.show()\\r\\n\\tprint()\\r\\n\\tspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(spark_dataframe._jdf, dataset_name, component_name, 20501)\\r\\n\\r\\nspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.deleteDatasetInfo(json.dumps(ls),20501,component_name)\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":7,\"componentName\":\"Code 7\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\nimport re\\r\\nimport os\\r\\nimport json\\r\\nimport boto3 as boto3\\r\\nimport pandas as pd\\r\\nfrom collections import defaultdict\\r\\nfrom pyspark.sql.types import *\\r\\n\\r\\nsheet_names \\u003d None\\r\\nif sheet_names is None or type(sheet_names) \\u003d\\u003d list:\\r\\n\\tif type(sheet_names) \\u003d\\u003d list:\\r\\n\\t\\tif len(sheet_names)\\u003d\\u003d0:\\r\\n\\t\\t\\traise Exception(\\\"sheet names should not be empty list\\\")\\r\\nelse:\\r\\n\\traise Exception(\\\"sheet names should be list or None\\\")\\r\\n\\r\\njava_object \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getConnectionParams(\\\"EXCEL aws s3 upendar\\\",\\\"read\\\")\\r\\n\\r\\ndata_source_details_list \\u003d [java_object.toList().apply(i) for i in range(java_object.size())]\\r\\ndata_source_details\\u003d {i._1(): i._2() for i in data_source_details_list}\\r\\n\\r\\naccess_key \\u003d data_source_details[\\\"aws-access-key\\\"]\\r\\nsecret_key \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getDecryptedPassword(data_source_details[\\\"aws-secret-key\\\"])\\r\\nregion_name \\u003d data_source_details.get(\\\"region\\\")\\r\\nbucket_name \\u003d data_source_details[\\\"aws-bucket-name\\\"]\\r\\nfile_path \\u003d os.path.join(data_source_details[\\\"path\\\"], \\\"sample1.xls\\\")\\r\\nheader \\u003d 0 if data_source_details.get(\\\"useHeader\\\")\\u003d\\u003d\\\"true\\\" else None\\r\\n\\r\\ns3 \\u003d boto3.resource(\\r\\n\\tservice_name\\u003d\\u0027s3\\u0027,\\r\\n\\tregion_name\\u003dregion_name,\\r\\n\\taws_access_key_id\\u003daccess_key,\\r\\n\\taws_secret_access_key\\u003dsecret_key\\r\\n)\\r\\nobj \\u003d s3.Bucket(bucket_name).Object(file_path).get()\\r\\ndownloaded_file \\u003d obj[\\u0027Body\\u0027].read()\\r\\ndf_dict \\u003d pd.read_excel(downloaded_file, sheet_name\\u003dsheet_names, header\\u003dheader, keep_default_na\\u003dFalse)\\r\\n\\r\\nprint()\\r\\nls\\u003d[]\\r\\ncomponent_name \\u003d \\\"Plugin 2\\\"\\r\\nfor sheet_name, dataframe in df_dict.items():\\r\\n\\tif dataframe.empty:\\r\\n\\t\\temp_rdd \\u003d spark.sparkContext.emptyRDD()\\r\\n\\t\\tcolumns \\u003d StructType([])\\r\\n\\t\\tspark_dataframe \\u003d spark.createDataFrame(data\\u003demp_rdd,\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tschema\\u003dcolumns)\\r\\n\\telse:\\r\\n\\t    data_list \\u003d dataframe.to_dict(orient\\u003d\\\"records\\\")\\r\\n\\t    rdd \\u003d spark.sparkContext.parallelize(data_list)\\r\\n\\t    spark_dataframe \\u003d spark.read.json(rdd.map(lambda x: json.dumps(x)))\\r\\n\\tdataset_name \\u003d re.sub(\\\"[^0-9A-Za-z_]\\\", \\\"_\\\", f\\\"{component_name}_{sheet_name}\\\")\\r\\n\\tcolumn_map1 \\u003d defaultdict(list)\\r\\n\\tcolumn_map2 \\u003d {}\\r\\n\\tfor column in spark_dataframe.columns:\\r\\n\\t\\tm_column \\u003d re.sub(\\u0027[^0-9A-Za-z_]\\u0027, \\u0027_\\u0027, column)\\r\\n\\t\\tcolumn_map1[m_column].append(column)\\r\\n\\t\\tcolumn_map2[column] \\u003d m_column\\r\\n\\tfor n_column, o_columns in column_map1.items():\\r\\n\\t\\tif len(o_columns) \\u003e 1:\\r\\n\\t\\t\\tfor num, o_col in enumerate(o_columns):\\r\\n\\t\\t\\t\\tcolumn_map2[o_col] \\u003d f\\\"{n_column}_{num + 1}\\\"\\r\\n\\tspark_dataframe \\u003d spark_dataframe.toDF(*[column_map2[c] for c in spark_dataframe.columns])\\r\\n\\r\\n\\tspark_dataframe.createOrReplaceTempView(dataset_name)\\r\\n\\tprint(dataset_name)\\r\\n\\tls.append(dataset_name)\\r\\n\\tspark_dataframe.show()\\r\\n\\tprint()\\r\\n\\tspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(spark_dataframe._jdf, dataset_name, component_name, 20501)\\r\\n\\r\\nspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.deleteDatasetInfo(json.dumps(ls),20501,component_name)\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":8,\"componentName\":\"Code 8\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":8,"livyOptions":"{\"proxyUser\":\"\",\"jars\":[],\"pyFiles\":[],\"files\":[],\"driverMemory\":\"\",\"driverCores\":0,\"executorMemory\":\"\",\"executorCores\":0,\"numExecutors\":0,\"archives\":[],\"queue\":\"\",\"name\":\"\",\"conf\":{},\"heartbeatTimeoutInSecond\":0,\"kind\":\"spark\"}","isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow/upendar","workSchemaName":null},"analysis":[],"datamodels":[],"tagDetails":[],"dataCompares":[]}