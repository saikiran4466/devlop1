{"dataflow":{"dfKey":"b0cddc99-8b4d-46e9-aaa6-9ebdf1e06729","name":"dataset_flatten_test","tags":null,"description":null,"definition":"{\"name\":\"dataset_flatten_test\",\"version\":0,\"livyServerId\":0,\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"format\":\"json\",\"enableSchema\":\"N\",\"schema\":[],\"fileMetadata\":[],\"enableTrim\":\"N\",\"componentId\":2,\"componentName\":\"File 2\",\"tableName\":\"File_2\",\"category\":\"Source\",\"componentType\":\"File\",\"rank\":0,\"dataSourceName\":\"JSON_upendar\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"mode\":\"PERMISSIVE\",\"dateFormat\":\"yyyy-MM-dd\",\"multiLine\":\"true\",\"timestampFormat\":\"yyyy-MM-dd \\u0027T\\u0027 HH:mm:ss.SSSXXX\",\"path\":\"Nebula_usecase2JSON-Course.json\",\"inferSchema\":\"true\"},\"className\":\"com.datagaps.dataflow.models.FileComponent\",\"isCheckpointEnabled\":\"N\",\"dataSourceLogicalName\":\"\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"format\":\"json\",\"enableSchema\":\"N\",\"schema\":[],\"fileMetadata\":[],\"enableTrim\":\"N\",\"componentId\":6,\"componentName\":\"File 6\",\"tableName\":\"File_6\",\"category\":\"Source\",\"componentType\":\"File\",\"rank\":0,\"dataSourceName\":\"JSON_upendar\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"mode\":\"PERMISSIVE\",\"dateFormat\":\"yyyy-MM-dd\",\"multiLine\":\"true\",\"timestampFormat\":\"yyyy-MM-dd \\u0027T\\u0027 HH:mm:ss.SSSXXX\",\"path\":\"customer.json\",\"inferSchema\":\"true\"},\"className\":\"com.datagaps.dataflow.models.FileComponent\",\"isCheckpointEnabled\":\"N\",\"dataSourceLogicalName\":\"\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"format\":\"json\",\"enableSchema\":\"N\",\"schema\":[],\"fileMetadata\":[],\"enableTrim\":\"N\",\"componentId\":9,\"componentName\":\"File 9\",\"tableName\":\"File_9\",\"category\":\"Source\",\"componentType\":\"File\",\"rank\":0,\"dataSourceName\":\"JSON_upendar\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"mode\":\"PERMISSIVE\",\"dateFormat\":\"yyyy-MM-dd\",\"multiLine\":\"true\",\"timestampFormat\":\"yyyy-MM-dd \\u0027T\\u0027 HH:mm:ss.SSSXXX\",\"path\":\"Multinode_100records.json\",\"inferSchema\":\"true\"},\"className\":\"com.datagaps.dataflow.models.FileComponent\",\"isCheckpointEnabled\":\"N\",\"dataSourceLogicalName\":\"\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from datagaps_utilities import dataset_flatten\\ndataset_flatten(session_object\\u003dspark,input_dataset\\u003d \\\"File_2\\\", component_name\\u003d\\\"Code 3\\\", dataflow_run_id\\u003d$[dataflow_run_id], output_dataset_primary\\u003d\\\"out1\\\")\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":3,\"componentName\":\"Code 3\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[2],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"from datagaps_utilities import dataset_flatten\\ndataset_flatten(session_object\\u003dspark,input_dataset\\u003d \\\"File_2\\\", component_name\\u003d\\\"Code 4\\\", dataflow_run_id\\u003d$[dataflow_run_id], output_dataset_primary\\u003d\\\"code_4_out1\\\", result_type\\u003d\\\"split\\\", unique_columns\\u003d\\\"transactionSource\\\", output_dataset_flattened\\u003d\\\"code_4_out2\\\")\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":4,\"componentName\":\"Code 4\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[2],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"from datagaps_utilities import dataset_flatten\\ndataset_flatten(session_object\\u003dspark,input_dataset\\u003d \\\"File_6\\\", component_name\\u003d\\\"issue_test_I317\\\", dataflow_run_id\\u003d$[dataflow_run_id], output_dataset_primary\\u003d\\\"out7_1\\\")\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":7,\"componentName\":\"issue_test_I317\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[6],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"},{\"code\":\"import json\\r\\nimport copy\\r\\nfrom pyspark.sql.functions import col\\r\\nimport traceback\\r\\n\\r\\ntry:\\r\\n    def dataset_flatten(session_object, component_name, dataflow_run_id, input_dataset, output_dataset_primary,\\r\\n                        result_type\\u003d\\\"single\\\", unique_columns\\u003dNone, output_dataset_flattened\\u003dNone):\\r\\n        \\\"\\\"\\\"\\r\\n        function to convert nested json type dataset to flattened dataset\\r\\n    \\r\\n        :param session_object:spark session object from code component\\r\\n        :param component_name: name of code component or plugin component\\r\\n        :param dataflow_run_id: dataflow run id by using datagaps inbuilt parameter $[dataflow_run_id]\\r\\n        :param input_dataset: name of the input dataset which we want to flatten\\r\\n        :param output_dataset_primary: name of the output dataset\\r\\n        :param result_type: if value is single generates one output dataset, if value is split generates two output\\r\\n        datasets, default is single.\\r\\n        :param unique_columns: required for result_type is split, if multiple columns should be separate with comma(,)\\r\\n        :param output_dataset_flattened: required for result_type is split, name of the second output dataset which\\r\\n        generates from first layer of non object columns\\r\\n        :return: None\\r\\n        \\\"\\\"\\\"\\r\\n        sdf \\u003d session_object.sql(f\\\"select * from {input_dataset}\\\")\\r\\n        if result_type not in [\\\"single\\\", \\\"split\\\"]:\\r\\n            raise Exception(\\\"result type should be single or split.\\\")\\r\\n        if result_type \\u003d\\u003d \\\"split\\\":\\r\\n            if output_dataset_flattened is None:\\r\\n                raise Exception(\\\"output_dataset_flattened is required for split result type.\\\")\\r\\n            if not unique_columns:\\r\\n                raise Exception(\\\"unique_columns are required for split result type.\\\")\\r\\n    \\r\\n        if result_type \\u003d\\u003d \\\"split\\\":\\r\\n            session_object.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.deleteDatasetInfo(json.dumps([\\r\\n                output_dataset_primary, output_dataset_flattened]),\\r\\n                dataflow_run_id, component_name)\\r\\n    \\r\\n            schema \\u003d [json.loads(i.json()) for i in sdf.schema.fields]\\r\\n            unique_cols \\u003d [i.strip() for i in unique_columns.split(\\\",\\\")]\\r\\n            object_cols \\u003d [col(i[\\\"name\\\"]) for i in schema if type(i[\\\"type\\\"]) !\\u003d str]\\r\\n            non_object_cols \\u003d [col(i[\\\"name\\\"]) for i in schema if type(i[\\\"type\\\"]) \\u003d\\u003d str]\\r\\n            non_object_col_names \\u003d [i[\\\"name\\\"] for i in schema if type(i[\\\"type\\\"]) \\u003d\\u003d str]\\r\\n            all_col_names \\u003d [i[\\\"name\\\"] for i in schema]\\r\\n            print(unique_cols)\\r\\n            print(all_col_names)\\r\\n    \\r\\n            for unique_col in unique_cols:\\r\\n                if unique_col not in all_col_names:\\r\\n                    raise Exception(f\\\"Column name \\u0027{unique_col}\\u0027 not in Json, Please provide exact column name.\\\")\\r\\n            if not set(unique_cols) \\u003c\\u003d set(non_object_col_names):\\r\\n                raise Exception(\\\"Unique columns should not be object type\\\")\\r\\n            table_1 \\u003d sdf.select(*unique_cols, *object_cols)\\r\\n            table_2 \\u003d sdf.select(*non_object_cols)\\r\\n            del sdf\\r\\n            del schema\\r\\n            jd \\u003d table_1.toJSON().collect()\\r\\n            del table_1\\r\\n        else:\\r\\n            jd \\u003d sdf.toJSON().collect()\\r\\n            del sdf\\r\\n        json_list \\u003d [json.loads(i) for i in jd]\\r\\n        del jd\\r\\n    \\r\\n        def flatten(d, parent\\u003dNone):\\r\\n            fd \\u003d {}\\r\\n            for k, v in d.items():\\r\\n                if parent:\\r\\n                    pre_fix \\u003d parent + \\\"_\\\" + k\\r\\n                else:\\r\\n                    pre_fix \\u003d k\\r\\n                if isinstance(v, dict):\\r\\n                    fd.update(flatten(v, pre_fix))\\r\\n                else:\\r\\n                    fd[pre_fix] \\u003d v\\r\\n            return fd\\r\\n    \\r\\n        def unwind(d):\\r\\n            recheck \\u003d False\\r\\n            l \\u003d []\\r\\n            for k, v in d.items():\\r\\n                if isinstance(v, list):\\r\\n                    if v:\\r\\n                        l.extend([{**copy.deepcopy(d), k: _} for _ in v])\\r\\n                    else:\\r\\n                        l.append({**copy.deepcopy(d), k: v})\\r\\n                    recheck \\u003d True\\r\\n                    break\\r\\n            else:\\r\\n                return [d]\\r\\n            if recheck:\\r\\n                if len(l) \\u003e 1:\\r\\n                    for i in range(len(l)):\\r\\n                        l.extend(unwind(l.pop(0)))\\r\\n            return l\\r\\n    \\r\\n        def final_1(dorl):\\r\\n            if isinstance(dorl, dict):\\r\\n                s \\u003d flatten(dorl)\\r\\n                res_unwind \\u003d unwind(s)\\r\\n                if res_unwind \\u003d\\u003d [dorl]:\\r\\n                    return res_unwind\\r\\n                else:\\r\\n                    res_unwind \\u003d final_1(res_unwind)\\r\\n            else:\\r\\n                res_unwind \\u003d []\\r\\n                for dl in dorl:\\r\\n                    res_unwind.extend(final_1(dl))\\r\\n            return res_unwind\\r\\n    \\r\\n        final_res \\u003d final_1(json_list)\\r\\n        del json_list\\r\\n        rdd \\u003d session_object.sparkContext.parallelize(final_res)\\r\\n        del final_res\\r\\n        final_df \\u003d session_object.read.json(rdd.map(lambda x: json.dumps(x)))\\r\\n    \\r\\n        final_df.createOrReplaceTempView(output_dataset_primary)\\r\\n        print(f\\\"{output_dataset_primary}: \\\")\\r\\n        final_df.show(3)\\r\\n        if result_type \\u003d\\u003d \\\"split\\\":\\r\\n            session_object.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(final_df._jdf,\\r\\n                                                                                                  output_dataset_primary,\\r\\n                                                                                                  component_name,\\r\\n                                                                                                  dataflow_run_id)\\r\\n            del final_df\\r\\n            table_2.createOrReplaceTempView(output_dataset_flattened)\\r\\n            print(f\\\"{output_dataset_flattened}: \\\")\\r\\n            table_2.show(3)\\r\\n            session_object.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(table_2._jdf,\\r\\n                                                                                                  output_dataset_flattened,\\r\\n                                                                                                  component_name,\\r\\n                                                                                                  dataflow_run_id)\\r\\n                                                                                                  \\r\\n    dataset_flatten(session_object\\u003dspark,input_dataset\\u003d \\\"File_9\\\", component_name\\u003d\\\"issue_test_I316\\\", dataflow_run_id\\u003d$[dataflow_run_id], output_dataset_primary\\u003d\\\"I316_out1\\\", result_type\\u003d\\\"split\\\", unique_columns\\u003d\\\"totalFeed\\\", output_dataset_flattened\\u003d\\\"I316_out2\\\")\\r\\n    \\r\\nexcept:\\r\\n    print(traceback.format_exc())\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":10,\"componentName\":\"issue_test_I316\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[9],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":10,"livyOptions":"{\"proxyUser\":\"\",\"jars\":[],\"pyFiles\":[],\"files\":[],\"driverMemory\":\"\",\"driverCores\":0,\"executorMemory\":\"\",\"executorCores\":0,\"numExecutors\":0,\"archives\":[],\"queue\":\"\",\"name\":\"\",\"conf\":{},\"heartbeatTimeoutInSecond\":0,\"kind\":\"spark\"}","isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow/upendar","workSchemaName":null},"analysis":[],"datamodels":[],"tagDetails":[],"dataCompares":[]}